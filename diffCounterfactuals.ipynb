{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_27152\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_27291\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_28459\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_28510\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_28561\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_28641\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_28749\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_28754\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_28875\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_28925\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_29056\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_29075\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_29224\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_29242\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_29263\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_29273\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_29299\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_29300\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_29311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'timestep_values': None, 'timesteps': 1000} were passed to DDIMScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-3.0893, device='cuda:0')\n",
      "tensor(-2.2242, device='cuda:0')\n",
      "tensor(-1.4349, device='cuda:0')\n",
      "tensor(-3.0878, device='cuda:0')\n",
      "tensor(-0.5016, device='cuda:0')\n",
      "tensor(-0.9032, device='cuda:0')\n",
      "tensor(-1.8061, device='cuda:0')\n",
      "tensor(-1.5006, device='cuda:0')\n",
      "tensor(-1.5091, device='cuda:0')\n",
      "tensor(0.5627, device='cuda:0')\n",
      "tensor(-3.0440, device='cuda:0')\n",
      "tensor(-1.7235, device='cuda:0')\n",
      "tensor(-2.6975, device='cuda:0')\n",
      "tensor(-1.1172, device='cuda:0')\n",
      "tensor(-0.9103, device='cuda:0')\n",
      "tensor(0.8591, device='cuda:0')\n",
      "tensor(-1.3892, device='cuda:0')\n",
      "tensor(-0.6905, device='cuda:0')\n",
      "tensor(-1.6558, device='cuda:0')\n",
      "tensor([False, False, False, False, False, False, False, False, False,  True,\n",
      "        False, False, False, False, False,  True, False, False, False])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 38.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -3.2522175312042236\n",
      "0 loss: 5.699406623840332 lr: [0.01] l1-dist 0.0447189137339592\n",
      "Classifier prediction: -2.5887632369995117\n",
      "2 loss: 5.0674028396606445 lr: [0.009931100837462445] l1-dist 0.047863952815532684\n",
      "Classifier prediction: -2.0944790840148926\n",
      "4 loss: 4.670379638671875 lr: [0.009774869058090914] l1-dist 0.0575900636613369\n",
      "Classifier prediction: -1.5859897136688232\n",
      "6 loss: 4.23384952545166 lr: [0.009543642776065642] l1-dist 0.06478600203990936\n",
      "Classifier prediction: -1.0579445362091064\n",
      "8 loss: 3.7961385250091553 lr: [0.009241066670644704] l1-dist 0.07381940633058548\n",
      "Classifier prediction: -0.43256035447120667\n",
      "10 loss: 3.2947678565979004 lr: [0.008871910576983217] l1-dist 0.08622074127197266\n",
      "Classifier prediction: 0.36656710505485535\n",
      "12 loss: 2.682431697845459 lr: [0.008441994226264132] l1-dist 0.10489988327026367\n",
      "Diffusion Counterfactual generated with loss: 2.682431697845459 | classifier_prediction: 0.36656710505485535 | l1_dist: 0.10489988327026367 | in 13 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/19 [00:29<08:53, 29.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 40.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -2.318789482116699\n",
      "0 loss: 4.804568290710449 lr: [0.01] l1-dist 0.04857790097594261\n",
      "Classifier prediction: -1.6148781776428223\n",
      "2 loss: 4.227785110473633 lr: [0.009931100837462445] l1-dist 0.061290718615055084\n",
      "Classifier prediction: -1.0618886947631836\n",
      "4 loss: 3.7591583728790283 lr: [0.009774869058090914] l1-dist 0.06972696632146835\n",
      "Classifier prediction: -0.47366341948509216\n",
      "6 loss: 3.306386947631836 lr: [0.009543642776065642] l1-dist 0.08327235281467438\n",
      "Classifier prediction: 0.17199984192848206\n",
      "8 loss: 2.8316407203674316 lr: [0.009241066670644704] l1-dist 0.10036405920982361\n",
      "Diffusion Counterfactual generated with loss: 2.8316407203674316 | classifier_prediction: 0.17199984192848206 | l1_dist: 0.10036405920982361 | in 9 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2/19 [00:50<07:00, 24.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 41.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.474960207939148\n",
      "0 loss: 4.118856906890869 lr: [0.01] l1-dist 0.06438963860273361\n",
      "Classifier prediction: -0.9383646845817566\n",
      "2 loss: 3.5223708152770996 lr: [0.009931100837462445] l1-dist 0.05840061604976654\n",
      "Classifier prediction: -0.40189120173454285\n",
      "4 loss: 3.0802111625671387 lr: [0.009774869058090914] l1-dist 0.06783197820186615\n",
      "Classifier prediction: 0.04470425099134445\n",
      "6 loss: 2.707106590270996 lr: [0.009543642776065642] l1-dist 0.07518108934164047\n",
      "Diffusion Counterfactual generated with loss: 2.707106590270996 | classifier_prediction: 0.04470425099134445 | l1_dist: 0.07518108934164047 | in 7 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 3/19 [01:08<05:40, 21.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 35.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -3.186439275741577\n",
      "0 loss: 5.549551486968994 lr: [0.01] l1-dist 0.03631121292710304\n",
      "Classifier prediction: -2.553800582885742\n",
      "2 loss: 4.9681196212768555 lr: [0.009931100837462445] l1-dist 0.04143190383911133\n",
      "Classifier prediction: -1.9538724422454834\n",
      "4 loss: 4.52001953125 lr: [0.009774869058090914] l1-dist 0.05661468952894211\n",
      "Classifier prediction: -1.4631935358047485\n",
      "6 loss: 4.133827209472656 lr: [0.009543642776065642] l1-dist 0.06706339865922928\n",
      "Classifier prediction: -0.7307223677635193\n",
      "8 loss: 3.490025520324707 lr: [0.009241066670644704] l1-dist 0.07593031972646713\n",
      "Classifier prediction: 0.03512641042470932\n",
      "10 loss: 2.811046838760376 lr: [0.008871910576983217] l1-dist 0.08461733162403107\n",
      "Diffusion Counterfactual generated with loss: 2.811046838760376 | classifier_prediction: 0.03512641042470932 | l1_dist: 0.08461733162403107 | in 11 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 4/19 [01:33<05:45, 23.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 40.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -0.4957011044025421\n",
      "0 loss: 2.8765904903411865 lr: [0.01] l1-dist 0.03808894008398056\n",
      "Classifier prediction: 0.1650649607181549\n",
      "2 loss: 2.517275333404541 lr: [0.009931100837462445] l1-dist 0.06823403388261795\n",
      "Diffusion Counterfactual generated with loss: 2.517275333404541 | classifier_prediction: 0.1650649607181549 | l1_dist: 0.06823403388261795 | in 3 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 5/19 [01:43<04:12, 18.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 41.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -0.8376410007476807\n",
      "0 loss: 3.197890281677246 lr: [0.01] l1-dist 0.03602492809295654\n",
      "Classifier prediction: 0.08676400035619736\n",
      "2 loss: 2.4435505867004395 lr: [0.009931100837462445] l1-dist 0.05303145572543144\n",
      "Diffusion Counterfactual generated with loss: 2.4435505867004395 | classifier_prediction: 0.08676400035619736 | l1_dist: 0.05303145572543144 | in 3 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 6/19 [01:52<03:14, 14.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 41.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.9193310737609863\n",
      "0 loss: 4.337862491607666 lr: [0.01] l1-dist 0.04185314103960991\n",
      "Classifier prediction: -1.170490026473999\n",
      "2 loss: 3.656827926635742 lr: [0.009931100837462445] l1-dist 0.04863378405570984\n",
      "Classifier prediction: -0.5744560360908508\n",
      "4 loss: 3.4269518852233887 lr: [0.009774869058090914] l1-dist 0.08524959534406662\n",
      "Classifier prediction: 0.06149240583181381\n",
      "6 loss: 2.6546671390533447 lr: [0.009543642776065642] l1-dist 0.07161596417427063\n",
      "Diffusion Counterfactual generated with loss: 2.6546671390533447 | classifier_prediction: 0.06149240583181381 | l1_dist: 0.07161596417427063 | in 7 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 7/19 [02:08<03:06, 15.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 42.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.6750441789627075\n",
      "0 loss: 4.079682350158691 lr: [0.01] l1-dist 0.04046384245157242\n",
      "Classifier prediction: -1.4980902671813965\n",
      "2 loss: 4.777265548706055 lr: [0.009931100837462445] l1-dist 0.1279175579547882\n",
      "Diffusion Counterfactual generated with loss: 3.1036183834075928 | classifier_prediction: 0.567772388458252 | l1_dist: 0.16713908314704895 | in 4 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 8/19 [02:19<02:34, 14.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 37.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.632059097290039\n",
      "0 loss: 4.041840076446533 lr: [0.01] l1-dist 0.0409780852496624\n",
      "Classifier prediction: -0.6064081192016602\n",
      "2 loss: 3.2814807891845703 lr: [0.009931100837462445] l1-dist 0.06750728189945221\n",
      "Classifier prediction: 0.11454679816961288\n",
      "4 loss: 2.635000705718994 lr: [0.009774869058090914] l1-dist 0.07495476305484772\n",
      "Diffusion Counterfactual generated with loss: 2.635000705718994 | classifier_prediction: 0.11454679816961288 | l1_dist: 0.07495476305484772 | in 5 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 9/19 [02:32<02:17, 13.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 42.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: 0.5189474821090698\n",
      "0 loss: 1.802379846572876 lr: [0.01] l1-dist 0.03213272988796234\n",
      "Diffusion Counterfactual generated with loss: 1.802379846572876 | classifier_prediction: 0.5189474821090698 | l1_dist: 0.03213272988796234 | in 1 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 10/19 [02:37<01:38, 11.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 42.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -3.263070583343506\n",
      "0 loss: 5.676961898803711 lr: [0.01] l1-dist 0.04138915613293648\n",
      "Classifier prediction: -2.7082138061523438\n",
      "2 loss: 5.157123565673828 lr: [0.009931100837462445] l1-dist 0.04489099979400635\n",
      "Classifier prediction: -2.270517587661743\n",
      "4 loss: 4.784476280212402 lr: [0.009774869058090914] l1-dist 0.05139588564634323\n",
      "Classifier prediction: -1.84171462059021\n",
      "6 loss: 4.428225994110107 lr: [0.009543642776065642] l1-dist 0.05865113437175751\n",
      "Classifier prediction: -1.4122196435928345\n",
      "8 loss: 4.080976486206055 lr: [0.009241066670644704] l1-dist 0.06687569618225098\n",
      "Classifier prediction: -1.0332292318344116\n",
      "10 loss: 3.7843105792999268 lr: [0.008871910576983217] l1-dist 0.0751081183552742\n",
      "Classifier prediction: 3.389502763748169\n",
      "12 loss: 2.959108829498291 lr: [0.008441994226264132] l1-dist 0.156960591673851\n",
      "Diffusion Counterfactual generated with loss: 2.959108829498291 | classifier_prediction: 3.389502763748169 | l1_dist: 0.156960591673851 | in 13 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 11/19 [03:05<02:10, 16.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 42.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.825016736984253\n",
      "0 loss: 4.185042858123779 lr: [0.01] l1-dist 0.036002591252326965\n",
      "Classifier prediction: -1.3805158138275146\n",
      "2 loss: 3.9582600593566895 lr: [0.009931100837462445] l1-dist 0.057774417102336884\n",
      "Classifier prediction: -1.067775011062622\n",
      "4 loss: 3.786181926727295 lr: [0.009774869058090914] l1-dist 0.07184068858623505\n",
      "Classifier prediction: -0.6110727787017822\n",
      "6 loss: 3.450578212738037 lr: [0.009543642776065642] l1-dist 0.08395053446292877\n",
      "Classifier prediction: -0.04625455290079117\n",
      "8 loss: 2.9322566986083984 lr: [0.009241066670644704] l1-dist 0.08860021829605103\n",
      "Diffusion Counterfactual generated with loss: 2.7515997886657715 | classifier_prediction: 0.1602388322353363 | l1_dist: 0.0911838635802269 | in 10 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 12/19 [03:28<02:07, 18.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 41.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -2.889946460723877\n",
      "0 loss: 5.2585601806640625 lr: [0.01] l1-dist 0.036861352622509\n",
      "Classifier prediction: -1.5254398584365845\n",
      "2 loss: 4.340208530426025 lr: [0.009931100837462445] l1-dist 0.08147688955068588\n",
      "Classifier prediction: -0.14848747849464417\n",
      "4 loss: 3.214406728744507 lr: [0.009774869058090914] l1-dist 0.10659191012382507\n",
      "Diffusion Counterfactual generated with loss: 2.835378646850586 | classifier_prediction: 0.30666348338127136 | l1_dist: 0.11420422792434692 | in 6 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 13/19 [03:43<01:43, 17.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 41.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.1418135166168213\n",
      "0 loss: 3.5043153762817383 lr: [0.01] l1-dist 0.03625018522143364\n",
      "Classifier prediction: -0.5310899615287781\n",
      "2 loss: 3.1307365894317627 lr: [0.009931100837462445] l1-dist 0.059964656829833984\n",
      "Classifier prediction: -0.027157403528690338\n",
      "4 loss: 2.724557638168335 lr: [0.009774869058090914] l1-dist 0.06974003463983536\n",
      "Diffusion Counterfactual generated with loss: 2.527944564819336 | classifier_prediction: 0.20703992247581482 | l1_dist: 0.0734984427690506 | in 6 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 14/19 [03:58<01:23, 16.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 41.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -0.643534779548645\n",
      "0 loss: 3.346733570098877 lr: [0.01] l1-dist 0.07031987607479095\n",
      "Diffusion Counterfactual generated with loss: 1.7437050342559814 | classifier_prediction: 1.39066481590271 | l1_dist: 0.1134369820356369 | in 2 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 15/19 [04:05<00:54, 13.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 41.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: 0.886259138584137\n",
      "0 loss: 1.829973816871643 lr: [0.01] l1-dist 0.0716232880949974\n",
      "Diffusion Counterfactual generated with loss: 1.829973816871643 | classifier_prediction: 0.886259138584137 | l1_dist: 0.0716232880949974 | in 1 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 16/19 [04:10<00:33, 11.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 41.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.447609782218933\n",
      "0 loss: 3.9278740882873535 lr: [0.01] l1-dist 0.048026423901319504\n",
      "Classifier prediction: -0.045012153685092926\n",
      "2 loss: 2.6891040802001953 lr: [0.009931100837462445] l1-dist 0.06440918147563934\n",
      "Diffusion Counterfactual generated with loss: 2.326993942260742 | classifier_prediction: 0.4459528625011444 | l1_dist: 0.07729469239711761 | in 4 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 17/19 [04:21<00:22, 11.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 42.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -0.7701396346092224\n",
      "0 loss: 3.167670726776123 lr: [0.01] l1-dist 0.03975310176610947\n",
      "Classifier prediction: -0.21880844235420227\n",
      "2 loss: 2.6441030502319336 lr: [0.009931100837462445] l1-dist 0.042529474943876266\n",
      "Classifier prediction: 0.19607123732566833\n",
      "4 loss: 2.4090850353240967 lr: [0.009774869058090914] l1-dist 0.0605156235396862\n",
      "Diffusion Counterfactual generated with loss: 2.4090850353240967 | classifier_prediction: 0.19607123732566833 | l1_dist: 0.0605156235396862 | in 5 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 18/19 [04:34<00:11, 11.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 41.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.8123350143432617\n",
      "0 loss: 4.136782646179199 lr: [0.01] l1-dist 0.032444775104522705\n",
      "Classifier prediction: -0.7637918591499329\n",
      "2 loss: 3.321521759033203 lr: [0.009931100837462445] l1-dist 0.05577300116419792\n",
      "Classifier prediction: -0.052337922155857086\n",
      "4 loss: 2.9469642639160156 lr: [0.009774869058090914] l1-dist 0.08946263790130615\n",
      "Diffusion Counterfactual generated with loss: 2.7157416343688965 | classifier_prediction: 0.3060149848461151 | l1_dist: 0.10217565298080444 | in 6 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [04:49<00:00, 15.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import PIL.Image\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "from diffusers import UNet2DModel, DDIMScheduler, VQModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from pytorch_msssim import ssim, ms_ssim\n",
    "\n",
    "from zennit.composites import LayerMapComposite\n",
    "from zennit.rules import Epsilon, ZPlus, Pass, Norm\n",
    "\n",
    "from data.dataset import ImageDataset, CelebHQAttrDataset\n",
    "from init_classifier import LinearClassifier, VQVAEClassifier, ResNet50Classifier, ViTClassifier\n",
    "from xai_lrp import xai_zennit, show_attributions\n",
    "\n",
    "\n",
    "class CheckpointedUNetWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(CheckpointedUNetWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def checkpointed_forward(self, module, *inputs):\n",
    "        def custom_forward(*inputs):\n",
    "            return module(*inputs)\n",
    "        return checkpoint(custom_forward, *inputs)\n",
    "\n",
    "    def forward(self, sample, timestep):\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps * torch.ones(sample.shape[0], dtype=timesteps.dtype, device=timesteps.device)\n",
    "\n",
    "        t_emb = self.model.time_proj(timesteps)\n",
    "        #t_emb = t_emb.to(dtype=self.dtype)\n",
    "        emb = self.model.time_embedding(t_emb)\n",
    "\n",
    "        # 2. pre-process\n",
    "        skip_sample = sample\n",
    "        sample = self.model.conv_in(sample)\n",
    "\n",
    "        # 3. down\n",
    "        down_block_res_samples = (sample,)\n",
    "        for downsample_block in self.model.down_blocks:\n",
    "            if hasattr(downsample_block, \"skip_conv\"):\n",
    "                sample, res_samples, skip_sample = self.checkpointed_forward(downsample_block, sample, emb, skip_sample)\n",
    "            else:\n",
    "                sample, res_samples = self.checkpointed_forward(downsample_block, sample, emb)\n",
    "\n",
    "            down_block_res_samples += res_samples\n",
    "\n",
    "        # 4. mid\n",
    "        sample = self.checkpointed_forward(self.model.mid_block, sample, emb)\n",
    "\n",
    "        # 5. up\n",
    "        skip_sample = None\n",
    "        for upsample_block in self.model.up_blocks:\n",
    "            res_samples = down_block_res_samples[-len(upsample_block.resnets) :]\n",
    "            down_block_res_samples = down_block_res_samples[: -len(upsample_block.resnets)]\n",
    "\n",
    "            if hasattr(upsample_block, \"skip_conv\"):\n",
    "                sample, skip_sample = self.checkpointed_forward(upsample_block, sample, res_samples, emb, skip_sample)\n",
    "            else:\n",
    "                sample = self.checkpointed_forward(upsample_block, sample, res_samples, emb)\n",
    "\n",
    "        # 6. post-process\n",
    "        sample = self.model.conv_norm_out(sample)\n",
    "        sample = self.model.conv_act(sample)\n",
    "        sample = self.model.conv_out(sample)\n",
    "\n",
    "        if skip_sample is not None:\n",
    "            sample += skip_sample\n",
    "\n",
    "        return {\"sample\": sample}\n",
    "\n",
    "def classifier_loss(classifier, images, targets, idx):\n",
    "    preds = classifier(images)\n",
    "    if idx % 2 == 0:\n",
    "        print(f\"Classifier prediction: {preds[0][cls_id]}\")\n",
    "    targets = torch.tensor(targets).to(device)\n",
    "    #error = torch.nn.functional.binary_cross_entropy_with_logits(preds[0][31], targets)\n",
    "    error = torch.abs(preds[0][cls_id] - targets).mean()\n",
    "    preds_binary = torch.sigmoid(preds[0][cls_id]) > 0.5\n",
    "\n",
    "    return error, preds_binary\n",
    "\n",
    "def minDist_loss(counterfactual_images, original_images):\n",
    "    # l1 distance\n",
    "    error = torch.abs(counterfactual_images - original_images).mean()\n",
    "\n",
    "    # ssim distance\n",
    "    # normalize images\n",
    "    #original_images = (original_images + 1) / 2\n",
    "    #counterfactual_images = (counterfactual_images + 1) / 2\n",
    "    #error = 1 - ssim(original_images, counterfactual_images, data_range=1.0, size_average=True)\n",
    "    return error\n",
    "\n",
    "\n",
    "# data loading with ground truth no smiling\n",
    "data = ImageDataset('/home/dai/GPU-Student-2/Cederic/DataSciPro/data/misclsData_gt0', image_size=256, exts=['jpg', 'JPG', 'png'], do_augment=False, sort_names=True)\n",
    "dataloader = DataLoader(data, batch_size=1, shuffle=False)\n",
    "\n",
    "# create output folders\n",
    "directory_names = []\n",
    "for i, _ in enumerate(dataloader):\n",
    "    img_index = dataloader.dataset.paths[i].name.split('_')[0]\n",
    "    directory_name = os.path.join(\"/home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear\", f'folder_IMG_{img_index}')\n",
    "    directory_names.append(directory_name)\n",
    "    os.makedirs(directory_name, exist_ok=True)\n",
    "    print(f'Created directory: {directory_name}')\n",
    "\n",
    "#\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cls_type = 'linear'\n",
    "cls_id =  CelebHQAttrDataset.cls_to_id['Smiling']\n",
    "\n",
    "# load all models\n",
    "unet = UNet2DModel.from_pretrained(\"CompVis/ldm-celebahq-256\", subfolder=\"unet\")\n",
    "vqvae = VQModel.from_pretrained(\"CompVis/ldm-celebahq-256\", subfolder=\"vqvae\")\n",
    "scheduler = DDIMScheduler.from_config(\"CompVis/ldm-celebahq-256\", subfolder=\"scheduler\")\n",
    "\n",
    "unet.to(device)\n",
    "vqvae.to(device)\n",
    "\n",
    "checkpointed_unet = CheckpointedUNetWrapper(unet)\n",
    "\n",
    "# load all models\n",
    "if cls_type == 'linear':    \n",
    "    classifier = LinearClassifier.load_from_checkpoint(\"/home/dai/GPU-Student-2/Cederic/DataSciPro/cls_checkpoints/ffhq256.b128linear2024-06-02 13:08:28.ckpt\",\n",
    "                                            input_dim = data[0]['img'].shape,\n",
    "                                            num_classes = len(CelebHQAttrDataset.id_to_cls))\n",
    "elif cls_type == 'vqvae':\n",
    "    classifier = VQVAEClassifier.load_from_checkpoint(\"/home/dai/GPU-Student-2/Cederic/DataSciPro/cls_checkpoints/ffhq256.b32vqvae2024-06-01 08:48:59.ckpt\",\n",
    "                                       num_classes = len(CelebHQAttrDataset.id_to_cls))\n",
    "\n",
    "elif cls_type == 'res50':\n",
    "    classifier = ResNet50Classifier.load_from_checkpoint(\"/home/dai/GPU-Student-2/Cederic/DataSciPro/cls_checkpoints/ffhq256.b64res502024-06-02 17:06:41.ckpt\",\n",
    "                                            num_classes = len(CelebHQAttrDataset.id_to_cls))\n",
    "\n",
    "\n",
    "classifier.to(device)\n",
    "classifier.eval()\n",
    "# check functionality of classifier\n",
    "all_outputs = []\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['img'].to(classifier.device)\n",
    "        outputs = classifier(inputs)\n",
    "        print(outputs[0][cls_id])\n",
    "\n",
    "        preds_binary = torch.sigmoid(outputs[:, cls_id].cpu()) > 0.5\n",
    "        all_outputs.append(preds_binary) \n",
    "all_outputs = torch.cat(all_outputs, dim=0)\n",
    "print(all_outputs)\n",
    "\n",
    "\n",
    "###### explainable ai lrp\n",
    "# lrp rules\n",
    "layer_map_lrp_0 = [\n",
    "    (torch.nn.ReLU, Pass()),  # ignore activations\n",
    "    (torch.nn.Linear, Epsilon(epsilon=0)),  # this is the dense Linear, not any Linear\n",
    "    (torch.nn.Conv2d, ZPlus()),\n",
    "    (torch.nn.BatchNorm2d, Pass()),\n",
    "    (torch.nn.AdaptiveAvgPool2d, Norm()),\n",
    "]\n",
    "\n",
    "layer_map_lrp_zplus = [\n",
    "    (torch.nn.ReLU, Pass()),\n",
    "    (torch.nn.Linear, ZPlus()),  # this is the dense Linear, not any Linear\n",
    "    (torch.nn.Conv2d, ZPlus()),\n",
    "    (torch.nn.BatchNorm2d, Pass()),\n",
    "    (torch.nn.AdaptiveAvgPool2d, Norm()),\n",
    "]\n",
    "\n",
    "layer_map_lrp_eps = [\n",
    "    (torch.nn.ReLU, Pass()),\n",
    "    (torch.nn.Linear, Epsilon(epsilon=1)),  # this is the dense Linear, not any Linear\n",
    "    (torch.nn.Conv2d, ZPlus()),\n",
    "    (torch.nn.BatchNorm2d, Pass()),\n",
    "    (torch.nn.AdaptiveAvgPool2d, Norm()),\n",
    "]\n",
    "\n",
    "#before manipulation\n",
    "for i, batch in enumerate(dataloader):\n",
    "    inputs = batch['img'].to(classifier.device)\n",
    "    attr_znt_0 = [xai_zennit(classifier, inputs, RuleComposite=LayerMapComposite(layer_map_lrp_0), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    attr_znt_eps = [xai_zennit(classifier, inputs, RuleComposite=LayerMapComposite(layer_map_lrp_eps), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    attr_znt_zplus = [xai_zennit(classifier, inputs, RuleComposite=LayerMapComposite(layer_map_lrp_zplus), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    show_attributions(directory_names[i], attr_znt_0, title='Pre LRP-0')\n",
    "    #show_attributions(directory_names[i], attr_znt_eps, title='Pre LRP-EPS')\n",
    "    #show_attributions(directory_names[i], attr_znt_zplus, title='Pre LRP-Z+')\n",
    "        \n",
    "## Inversion\n",
    "def invert(\n",
    "    start_latents,\n",
    "    num_inference_steps,\n",
    "    device=device,\n",
    "):\n",
    "\n",
    "    # Latents are now the specified start latents\n",
    "    latents = start_latents.clone()\n",
    "\n",
    "    # We'll keep a list of the inverted latents as the process goes on\n",
    "    intermediate_latents = []\n",
    "\n",
    "    # Set num inference steps\n",
    "    scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "\n",
    "    # Reversed timesteps <<<<<<<<<<<<<<<<<<<<\n",
    "    timesteps = reversed(scheduler.timesteps)\n",
    "\n",
    "    for i in tqdm(range(1, num_inference_steps), total=num_inference_steps - 1):\n",
    "\n",
    "        # We'll skip the final iteration\n",
    "        if i >= num_inference_steps - 1:\n",
    "            continue\n",
    "\n",
    "        t = timesteps[i]\n",
    "\n",
    "        # Expand the latents if we are doing classifier free guidance\n",
    "        latent_model_input = latents\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "        # Predict the noise residual\n",
    "        noise_pred = checkpointed_unet(latent_model_input, t)[\"sample\"]\n",
    "\n",
    "        current_t = max(0, t.item() - (1000 // num_inference_steps))  # t\n",
    "        next_t = t  # min(999, t.item() + (1000//num_inference_steps)) # t+1\n",
    "        alpha_t = scheduler.alphas_cumprod[current_t]\n",
    "        alpha_t_next = scheduler.alphas_cumprod[next_t]\n",
    "\n",
    "        # Inverted update step (re-arranging the update step to get x(t) (new latents) as a function of x(t-1) (current latents)\n",
    "        latents = (latents - (1 - alpha_t).sqrt() * noise_pred) * (alpha_t_next.sqrt() / alpha_t.sqrt()) + (\n",
    "            1 - alpha_t_next\n",
    "        ).sqrt() * noise_pred\n",
    "\n",
    "        # Store\n",
    "        intermediate_latents.append(latents)\n",
    "\n",
    "    return torch.cat(intermediate_latents)\n",
    "\n",
    "\n",
    "class LatentNoise(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The LatentNoise Module makes it easier to update the noise tensor with torch optimizers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, noise: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.noise = torch.nn.Parameter(noise)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.noise\n",
    "\n",
    "\n",
    "def diffusion_pipe(noise_module: LatentNoise, num_inference_steps):\n",
    "        z = noise_module()\n",
    "        for i in range(start_step, num_inference_steps):\n",
    "            t = scheduler.timesteps[i]\n",
    "            z = scheduler.scale_model_input(z, t)\n",
    "            with torch.no_grad():\n",
    "                noise_pred = checkpointed_unet(z, t)[\"sample\"]\n",
    "            z = scheduler.step(noise_pred, t, z).prev_sample\n",
    "            z0 = scheduler.step(noise_pred, t, z).pred_original_sample\n",
    "        return z, z0\n",
    "\n",
    "def plot_images(images, titles=None, figsize=(50, 5), save_path=None):\n",
    "    n = len(images)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(n*5, 5))\n",
    "\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    #just for image sving\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        img.save(f\"{save_path}/{i}.png\")\n",
    "\n",
    "    #for i, img in enumerate(images):\n",
    "    #    axes[i].imshow(img)\n",
    "    #    axes[i].axis('off')\n",
    "    #    #if titles is not None:\n",
    "    #    #    axes[i].set_title(titles[i])\n",
    "#\n",
    "    #if save_path:\n",
    "    #    plt.savefig(save_path)\n",
    "    #plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_to_pil(tensor):\n",
    "    image = tensor.cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    image = PIL.Image.fromarray(np.array(image[0].detach().numpy() * 255).astype(np.uint8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def tensor_to_pil_image(tensor):\n",
    "    image = tensor.cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    image = PIL.Image.fromarray(np.array(image[0].detach().numpy() * 255).astype(np.uint8))\n",
    "    return image\n",
    "\n",
    "# conditional sampling\n",
    "num_inference_steps = 100\n",
    "start_step = 20\n",
    "\n",
    "\n",
    "for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "    #plot_to_pil(batch['img'])\n",
    "    with torch.no_grad():\n",
    "        z = vqvae.encode(batch['img'].to(device))   # encode the image in the latent space\n",
    "    z = z.latents\n",
    "    #plot_to_pil(z)\n",
    "    \n",
    "    #cond = z.view(1,-1)\n",
    "    #cond = normalize(cond)\n",
    "    #cond = cond + 0.5 * math.sqrt(512) * classifier.fc1.weight[31].unsqueeze(0)\n",
    "    #cond = denormalize(cond)\n",
    "    #z = cond.view(1,3,64,64)\n",
    "    #dec_z = vqvae.decode(z)[0]\n",
    "    #plot_to_pil(dec_z)\n",
    "    \n",
    "    inverted_latents = invert(z, num_inference_steps)                  # do the ddim scheduler reversed to add noise to the latents\n",
    "    z = inverted_latents[-(start_step + 1)].unsqueeze(0)                  # use these latents to start the sampling. better performance when using not the last latent sample\n",
    "    #plot_to_pil(z)\n",
    "    noise_module = LatentNoise(z.clone()).to(device)                    # convert latent noise to a parameter module for optimization\n",
    "    noise_module.noise.requires_grad = True\n",
    "    intermediate_results = [batch['img'].to(device)]   # list to store the results of the steering\n",
    "    intermediate_preds = [round(classifier(batch['img'].to(device))[0][cls_id].item(), 5)]\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        noise_module.parameters(), lr=0.01, maximize=False # not minimize gradient ascent\n",
    "    )\n",
    "    learning_scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "    \n",
    "    x = torch.zeros_like(z)\n",
    "    current_loss = float('inf')\n",
    "    preds_binary = False\n",
    "    current_pred = 0.0\n",
    "    i = 0\n",
    "    # for the linear classifier it works perfect to break out of the loop if the prediction switches.\n",
    "    #while (current_pred < 1.0) & (i < 20) :\n",
    "    while (preds_binary == False) & (i < 20) :\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            x, x0 = diffusion_pipe(noise_module, num_inference_steps) # forward\n",
    "            #plot_to_pil(x)\n",
    "            decoded_x = vqvae.decode(x)[0]\n",
    "            #plot_to_pil(decoded_x)\n",
    "            current_pred = classifier(decoded_x)[0][cls_id]\n",
    "\n",
    "            if i % 1 == 0:\n",
    "                intermediate_results.append(decoded_x)\n",
    "                intermediate_preds.append(round(current_pred.item(), 5))\n",
    "\n",
    "            loss, preds_binary = classifier_loss(classifier, decoded_x, 2.0, i)\n",
    "            l1_dist = minDist_loss(decoded_x, batch['img'].to(device))\n",
    "            #implementing the ssim and msssim distance\n",
    "            #ssim_dist = minDist_loss(decoded_x, batch['img'].to(device))\n",
    "\n",
    "            loss += l1_dist * 10\n",
    "            #loss += ssim_dist * 20\n",
    "            \n",
    "            if i % 2 == 0:\n",
    "                print(i, \"loss:\", loss.item(), \"lr:\", learning_scheduler.get_lr(), \"l1-dist\", l1_dist.item())\n",
    "                #print(i, \"loss:\", loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            learning_scheduler.step()\n",
    "\n",
    "            current_loss = loss.item()\n",
    "            i += 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image = vqvae.decode(x)[0]\n",
    "\n",
    "    print(f\"Diffusion Counterfactual generated with loss: {current_loss} | classifier_prediction: {current_pred} | l1_dist: {l1_dist} | in {i} optimization steps\")\n",
    "    \n",
    "    #lrp after manipulation\n",
    "    attr_znt_0 = [xai_zennit(classifier, image, RuleComposite=LayerMapComposite(layer_map_lrp_0), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    attr_znt_eps = [xai_zennit(classifier, image, RuleComposite=LayerMapComposite(layer_map_lrp_eps), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    attr_znt_zplus = [xai_zennit(classifier, image, RuleComposite=LayerMapComposite(layer_map_lrp_zplus), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    show_attributions(directory_names[step], attr_znt_0, title='Post LRP-0')\n",
    "    #show_attributions(directory_names[step], attr_znt_eps, title='Post LRP-EPS')\n",
    "    #show_attributions(directory_names[step], attr_znt_zplus, title='Post LRP-Z+')\n",
    "    image.requires_grad = False\n",
    "    \n",
    "    images = [tensor_to_pil_image(tensor) for tensor in intermediate_results]\n",
    "    gif_path = f\"{directory_names[step]}/GIF.gif\"\n",
    "    imageio.mimsave(gif_path, images, format='GIF', duration=10.0, loop=0)  # duration is in seconds\n",
    "\n",
    "    row_path = f\"{directory_names[step]}/sequence_small\"\n",
    "    plot_images(images, intermediate_preds, save_path=row_path)\n",
    "\n",
    "    # process image\n",
    "    image_processed = image.cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    image_pil = PIL.Image.fromarray(np.array(image_processed[0] * 255).astype(np.uint8))\n",
    "    ori_processed = batch['img'].cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    ori_image = PIL.Image.fromarray(np.array(ori_processed[0] * 255).astype(np.uint8))\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(image_pil)\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title('Diffusion Counterfactual Image')\n",
    "    axs[1].imshow(ori_image)\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title('Original Image')\n",
    "    #plt.show()\n",
    "    fig.savefig(f'{directory_names[step]}/ori_vs_DCE.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    image_pil.save(f\"{directory_names[step]}/diffCounter_IMG.png\")\n",
    "    print('finish')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffcounter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
