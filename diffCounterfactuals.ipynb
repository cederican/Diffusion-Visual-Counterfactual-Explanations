{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.2285, device='cuda:0')\n",
      "tensor([False])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dai/GPU-Student-2/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/dai/GPU-Student-2/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddim.DDIMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "The config attributes {'timestep_values': None, 'timesteps': 1000} were passed to DDIMScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]/home/dai/GPU-Student-2/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/dai/GPU-Student-2/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/dai/GPU-Student-2/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/49 [00:54<?, ?it/s]\n",
      "  0%|          | 0/1 [00:54<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [224, 3, 3, 3], expected input[1, 896, 8, 8] to have 3 channels, but got 896 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 247\u001b[0m\n\u001b[1;32m    244\u001b[0m     z \u001b[38;5;241m=\u001b[39m vqvae\u001b[38;5;241m.\u001b[39mencode(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device))   \u001b[38;5;66;03m# encode the image in the latent space\u001b[39;00m\n\u001b[1;32m    245\u001b[0m z \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39mlatents\n\u001b[0;32m--> 247\u001b[0m inverted_latents \u001b[38;5;241m=\u001b[39m \u001b[43minvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m                  \u001b[38;5;66;03m# do the ddim scheduler reversed to add noise to the latents\u001b[39;00m\n\u001b[1;32m    248\u001b[0m z \u001b[38;5;241m=\u001b[39m inverted_latents[\u001b[38;5;241m-\u001b[39m(start_step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)                  \u001b[38;5;66;03m# use these latents to start the sampling. better performance when using not the last latent sample\u001b[39;00m\n\u001b[1;32m    249\u001b[0m noise_module \u001b[38;5;241m=\u001b[39m LatentNoise(z\u001b[38;5;241m.\u001b[39mclone())\u001b[38;5;241m.\u001b[39mto(device)                    \u001b[38;5;66;03m# convert latent noise to a parameter module for optimization\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 180\u001b[0m, in \u001b[0;36minvert\u001b[0;34m(start_latents, guidance_scale, num_inference_steps, device)\u001b[0m\n\u001b[1;32m    177\u001b[0m latent_model_input \u001b[38;5;241m=\u001b[39m scheduler\u001b[38;5;241m.\u001b[39mscale_model_input(latent_model_input, t)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# Predict the noise residual\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcheckpointed_unet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m    182\u001b[0m current_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_inference_steps))  \u001b[38;5;66;03m# t\u001b[39;00m\n\u001b[1;32m    183\u001b[0m next_t \u001b[38;5;241m=\u001b[39m t  \u001b[38;5;66;03m# min(999, t.item() + (1000//num_inference_steps)) # t+1\u001b[39;00m\n",
      "File \u001b[0;32m~/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 67\u001b[0m, in \u001b[0;36mCheckpointedUNetWrapper.forward\u001b[0;34m(self, sample, timestep)\u001b[0m\n\u001b[1;32m     57\u001b[0m         sample, res_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointed_forward(downsample_block, sample, emb)\n\u001b[1;32m     59\u001b[0m     down_block_res_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res_samples\n\u001b[0;32m---> 67\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Apply checkpointing to down blocks\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdown_blocks:\n",
      "File \u001b[0;32m~/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [224, 3, 3, 3], expected input[1, 896, 8, 8] to have 3 channels, but got 896 channels instead"
     ]
    }
   ],
   "source": [
    "import PIL.Image\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "\n",
    "from diffusers import UNet2DModel, DDIMScheduler, VQModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from data.dataset import ImageDataset, CelebHQAttrDataset\n",
    "from init_classifier import Classifier\n",
    "\n",
    "\n",
    "class CheckpointedUNetWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(CheckpointedUNetWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def checkpointed_forward(self, module, *inputs):\n",
    "        def custom_forward(*inputs):\n",
    "            return module(*inputs)\n",
    "        return checkpoint(custom_forward, *inputs)\n",
    "\n",
    "    def forward(self, sample, timestep):\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps * torch.ones(sample.shape[0], dtype=timesteps.dtype, device=timesteps.device)\n",
    "\n",
    "        t_emb = self.model.time_proj(timesteps)\n",
    "        #t_emb = t_emb.to(dtype=self.dtype)\n",
    "        emb = self.model.time_embedding(t_emb)\n",
    "\n",
    "        # 2. pre-process\n",
    "        skip_sample = sample\n",
    "        sample = self.model.conv_in(sample)\n",
    "\n",
    "        # 3. down\n",
    "        down_block_res_samples = (sample,)\n",
    "        for downsample_block in self.model.down_blocks:\n",
    "            if hasattr(downsample_block, \"skip_conv\"):\n",
    "                sample, res_samples, skip_sample = self.checkpointed_forward(downsample_block, sample, emb, skip_sample)\n",
    "                #sample, res_samples, skip_sample = downsample_block(\n",
    "                #    hidden_states=sample, temb=emb, skip_sample=skip_sample\n",
    "                #)\n",
    "            else:\n",
    "                #sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n",
    "                sample, res_samples = self.checkpointed_forward(downsample_block, sample, emb)\n",
    "\n",
    "            down_block_res_samples += res_samples\n",
    "        \n",
    "        # 4. mid\n",
    "        sample = self.checkpointed_forward(self.model.mid_block, sample, emb)\n",
    "\n",
    "        # 5. up\n",
    "        skip_sample = None\n",
    "        for upsample_block in self.model.up_blocks:\n",
    "            res_samples = down_block_res_samples[-len(upsample_block.resnets) :]\n",
    "            down_block_res_samples = down_block_res_samples[: -len(upsample_block.resnets)]\n",
    "\n",
    "            if hasattr(upsample_block, \"skip_conv\"):\n",
    "                #sample, skip_sample = upsample_block(sample, res_samples, emb, skip_sample)\n",
    "            else:\n",
    "                #sample = upsample_block(sample, res_samples, emb)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        sample = self.model.conv_in(sample)\n",
    "\n",
    "        # Apply checkpointing to down blocks\n",
    "        for block in self.model.down_blocks:\n",
    "            sample = self.checkpointed_forward(block, sample, timestep)\n",
    "        \n",
    "        # Apply checkpointing to middle block\n",
    "        sample = self.checkpointed_forward(self.model.mid_block, sample, timestep)\n",
    "\n",
    "        # Apply checkpointing to up blocks\n",
    "        for block in self.model.up_blocks:\n",
    "            sample = self.checkpointed_forward(block, sample, timestep)\n",
    "        \n",
    "        # Apply conv_out layer\n",
    "        sample = self.model.conv_out(sample)\n",
    "        \n",
    "        return {\"sample\": sample}\n",
    "\n",
    "\n",
    "def color_loss(images, target_color=(1.0, 0.0, 0.0)):\n",
    "    \"\"\"Given a target color (R, G, B) return a loss for how far away on average\n",
    "    the images' pixels are from that color. Defaults to a light teal: (0.1, 0.9, 0.5)\"\"\"\n",
    "    target = torch.tensor(target_color).to(images.device) * 2 - 1  # Map target color to (-1, 1)\n",
    "    target = target[None, :, None, None]  # Get shape right to work with the images (b, c, h, w)\n",
    "    error = torch.abs(images - target).mean()  # Mean absolute difference between the image pixels and the target color\n",
    "    return error\n",
    "\n",
    "def classifier_loss(classifier, images, targets, idx):\n",
    "    preds = classifier(images)\n",
    "    if idx % 10 == 0:\n",
    "        print(f\"Classifier prediction: {preds[0][31]}\")\n",
    "    error = torch.abs(preds[0][31] - targets).mean()\n",
    "    return error\n",
    "\n",
    "def minDist_loss(images, original_images):\n",
    "    error = torch.abs(images - original_images).mean()\n",
    "    return error\n",
    "\n",
    "#enable checkpointing\n",
    "\n",
    "\n",
    "# data loading with ground truth no smiling\n",
    "data = ImageDataset('/home/dai/GPU-Student-2/Cederic/DataSciPro/data/misclsData_gt1', image_size=256, exts=['jpg', 'JPG', 'png'], do_augment=False, sort_names=True)\n",
    "dataloader = DataLoader(data, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "#\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"CompVis/ldm-celebahq-256\"\n",
    "seed = 3\n",
    "\n",
    "# load all models\n",
    "classifier = Classifier.load_from_checkpoint(\"/home/dai/GPU-Student-2/Cederic/pjds_group8/cls_checkpoints/ffhq256.2024-05-26 18:46:33.ckpt\",\n",
    "                                             input_dim = data[0]['img'].shape,\n",
    "                                             num_classes = len(CelebHQAttrDataset.id_to_cls))\n",
    "classifier.to(device)\n",
    "classifier.eval()\n",
    "# check functionality of classifier\n",
    "all_outputs = []\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['img'].to(classifier.device)\n",
    "        outputs = classifier(inputs)\n",
    "        print(outputs[0][31])\n",
    "\n",
    "        preds_binary = torch.sigmoid(outputs[:, CelebHQAttrDataset.cls_to_id['Smiling']].cpu()) > 0.5\n",
    "        all_outputs.append(preds_binary) \n",
    "all_outputs = torch.cat(all_outputs, dim=0)\n",
    "print(all_outputs)\n",
    "\n",
    "unet = UNet2DModel.from_pretrained(\"CompVis/ldm-celebahq-256\", subfolder=\"unet\")\n",
    "vqvae = VQModel.from_pretrained(\"CompVis/ldm-celebahq-256\", subfolder=\"vqvae\")\n",
    "scheduler = DDIMScheduler.from_config(\"CompVis/ldm-celebahq-256\", subfolder=\"scheduler\")\n",
    "scheduler.set_timesteps(num_inference_steps=25)\n",
    "\n",
    "unet.to(device)\n",
    "vqvae.to(device)\n",
    "\n",
    "checkpointed_unet = CheckpointedUNetWrapper(unet)\n",
    "\n",
    "## Inversion\n",
    "def invert(\n",
    "    start_latents,\n",
    "    guidance_scale=3.5,\n",
    "    num_inference_steps=50,\n",
    "    device=device,\n",
    "):\n",
    "\n",
    "    # Latents are now the specified start latents\n",
    "    latents = start_latents.clone()\n",
    "\n",
    "    # We'll keep a list of the inverted latents as the process goes on\n",
    "    intermediate_latents = []\n",
    "\n",
    "    # Set num inference steps\n",
    "    scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "\n",
    "    # Reversed timesteps <<<<<<<<<<<<<<<<<<<<\n",
    "    timesteps = reversed(scheduler.timesteps)\n",
    "\n",
    "    for i in tqdm(range(1, num_inference_steps), total=num_inference_steps - 1):\n",
    "\n",
    "        # We'll skip the final iteration\n",
    "        if i >= num_inference_steps - 1:\n",
    "            continue\n",
    "\n",
    "        t = timesteps[i]\n",
    "\n",
    "        # Expand the latents if we are doing classifier free guidance\n",
    "        latent_model_input = latents\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "        # Predict the noise residual\n",
    "        noise_pred = checkpointed_unet(latent_model_input, t).sample\n",
    "\n",
    "        current_t = max(0, t.item() - (1000 // num_inference_steps))  # t\n",
    "        next_t = t  # min(999, t.item() + (1000//num_inference_steps)) # t+1\n",
    "        alpha_t = scheduler.alphas_cumprod[current_t]\n",
    "        alpha_t_next = scheduler.alphas_cumprod[next_t]\n",
    "\n",
    "        # Inverted update step (re-arranging the update step to get x(t) (new latents) as a function of x(t-1) (current latents)\n",
    "        latents = (latents - (1 - alpha_t).sqrt() * noise_pred) * (alpha_t_next.sqrt() / alpha_t.sqrt()) + (\n",
    "            1 - alpha_t_next\n",
    "        ).sqrt() * noise_pred\n",
    "\n",
    "        # Store\n",
    "        intermediate_latents.append(latents)\n",
    "\n",
    "    return torch.cat(intermediate_latents)\n",
    "\n",
    "\n",
    "class LatentNoise(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The LatentNoise Module makes it easier to update the noise tensor with torch optimizers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, noise: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.noise = torch.nn.Parameter(noise)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.noise\n",
    "\n",
    "\n",
    "def diffusion_pipe(noise_module: LatentNoise, num_inference_steps):\n",
    "        z = noise_module()\n",
    "        for i in range(start_step, num_inference_steps):\n",
    "            t = scheduler.timesteps[i]\n",
    "            z = scheduler.scale_model_input(z, t)\n",
    "            with torch.no_grad():\n",
    "                noise_pred = unet(z, t)[\"sample\"]\n",
    "            z = scheduler.step(noise_pred, t, z).prev_sample\n",
    "        return z\n",
    "\n",
    "def plot_to_pil(tensor):\n",
    "    image = tensor.cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    image = PIL.Image.fromarray(np.array(image[0].detach().numpy() * 255).astype(np.uint8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def tensor_to_pil_image(tensor):\n",
    "    image = tensor.cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    image = PIL.Image.fromarray(np.array(image[0].detach().numpy() * 255).astype(np.uint8))\n",
    "    return image\n",
    "\n",
    "\n",
    "# conditional sampling\n",
    "guidance_cls = 2\n",
    "guidance_dist = 15\n",
    "num_inference_steps = 50\n",
    "num_optimization_steps = 50\n",
    "start_step = 10\n",
    "\n",
    "\n",
    "for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "    with torch.no_grad():\n",
    "        z = vqvae.encode(batch['img'].to(device))   # encode the image in the latent space\n",
    "    z = z.latents\n",
    "    \n",
    "    inverted_latents = invert(z, num_inference_steps=50)                  # do the ddim scheduler reversed to add noise to the latents\n",
    "    z = inverted_latents[-(start_step + 1)].unsqueeze(0)                  # use these latents to start the sampling. better performance when using not the last latent sample\n",
    "    noise_module = LatentNoise(z.clone()).to(device)                    # convert latent noise to a parameter module for optimization\n",
    "    noise_module.noise.requires_grad = True\n",
    "    intermediate_results = []   # list to store the results of the steering\n",
    "\n",
    "    #new_z = z.clone().detach().requires_grad_(True)\n",
    "\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        im = vqvae.decode(inverted_latents[-1].unsqueeze(0))\n",
    "        im_processed = im[0].cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "        im_pil = PIL.Image.fromarray(np.array(im_processed[0] * 255).astype(np.uint8))\n",
    "        plt.imshow(im_pil)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        noise_module.parameters(), lr=0.01, maximize=False # not minimize gradient ascent\n",
    "    )\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for i in tqdm(range(start_step, num_inference_steps)):\n",
    "\n",
    "        t = scheduler.timesteps[i]\n",
    "\n",
    "        # Prepare the model input\n",
    "        model_input = scheduler.scale_model_input(x, t)\n",
    "\n",
    "        # predict noise residual of previous image\n",
    "        with torch.no_grad():\n",
    "            noise_pred = unet(model_input, t)[\"sample\"]\n",
    "\n",
    "        # Set x.requires_grad to True\n",
    "        x = x.detach().requires_grad_()\n",
    "\n",
    "        # Get the predicted x0\n",
    "        x0 = scheduler.step(noise_pred, t, x).pred_original_sample\n",
    "        decoded_x0 = vqvae.decode(x0)[0]\n",
    "        decoded_x = vqvae.decode(x)[0]\n",
    "\n",
    "        # Calculate loss\n",
    "        #loss = color_loss(x0) * guidance_loss_scale\n",
    "        loss = guidance_cls * classifier_loss(classifier, decoded_x0, 1.0) + guidance_dist * minDist_loss(decoded_x0, decoded_x)\n",
    "        if i % 10 == 0:\n",
    "            print(i, \"loss:\", loss.item())\n",
    "\n",
    "        # Get gradient\n",
    "        cond_grad = -torch.autograd.grad(loss, x)[0]\n",
    "\n",
    "        # Modify x based on this gradient\n",
    "        x = x.detach() + cond_grad \n",
    "\n",
    "        # Now step with scheduler\n",
    "        x = scheduler.step(noise_pred, t, x).prev_sample\n",
    "\n",
    "    # decode image with vae\n",
    "    with torch.no_grad():\n",
    "        image = vqvae.decode(x)[0]\n",
    "    \"\"\"\n",
    "    x = torch.zeros_like(z)\n",
    "    for i in tqdm(range(num_optimization_steps)):\n",
    "            optimizer.zero_grad()\n",
    "            x = diffusion_pipe(noise_module, num_inference_steps) # forward\n",
    "            decoded_x = vqvae.decode(x)[0]\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                intermediate_results.append(decoded_x)\n",
    "            #    plot_to_pil(decoded_x)\n",
    "\n",
    "            loss = classifier_loss(classifier, decoded_x, 1.0, i) # apply losses\n",
    "            if i % 10 == 0:\n",
    "                print(i, \"loss:\", loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image = vqvae.decode(x)[0]\n",
    "    \n",
    "    images = [tensor_to_pil_image(tensor) for tensor in intermediate_results]\n",
    "    gif_path = \"/home/dai/GPU-Student-2/Cederic/DataSciPro/gifs/output.gif\"\n",
    "    imageio.mimsave(gif_path, images, format='GIF', duration=1.0, loop=0)  # duration is in seconds\n",
    "\n",
    "\n",
    "    # process image\n",
    "    image_processed = image.cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    image_pil = PIL.Image.fromarray(np.array(image_processed[0] * 255).astype(np.uint8))\n",
    "    ori_processed = batch['img'].cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    ori_image = PIL.Image.fromarray(np.array(ori_processed[0] * 255).astype(np.uint8))\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(image_pil)\n",
    "    axs[0].axis('off')\n",
    "    axs[1].imshow(ori_image)\n",
    "    axs[1].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    image_pil.save(f\"generated_image_{seed}.png\")\n",
    "    print('finish')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffcounter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
