{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_ResNet50/folder_IMG_28113\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_ResNet50/folder_IMG_28125\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_ResNet50/folder_IMG_28285\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_ResNet50/folder_IMG_28354\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_ResNet50/folder_IMG_29762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'timestep_values': None, 'timesteps': 1000} were passed to DDIMScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n",
      "/home/dai/GPU-Student-2/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dai/GPU-Student-2/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.3821, device='cuda:0')\n",
      "tensor(-1.1783, device='cuda:0')\n",
      "tensor(-0.4628, device='cuda:0')\n",
      "tensor(-0.4333, device='cuda:0')\n",
      "tensor(-1.5328, device='cuda:0')\n",
      "tensor([False, False, False, False, False])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 48.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -0.39776062965393066\n",
      "0 loss: 1.7811930179595947 lr: [0.01] l1-dist 0.038343243300914764\n",
      "Classifier prediction: 0.13450956344604492\n",
      "2 loss: 1.2896029949188232 lr: [0.009931100837462445] l1-dist 0.042411260306835175\n",
      "Classifier prediction: 0.002414185553789139\n",
      "4 loss: 1.8337554931640625 lr: [0.009774869058090914] l1-dist 0.0836169570684433\n",
      "Classifier prediction: 0.7990242838859558\n",
      "6 loss: 0.6252346634864807 lr: [0.009543642776065642] l1-dist 0.04242589324712753\n",
      "Diffusion Counterfactual generated with loss: 0.38714849948883057 | classifier_prediction: 1.0167787075042725 | l1_dist: 0.03703697770833969 | in 8 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:17<01:09, 17.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 48.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.1580030918121338\n",
      "0 loss: 2.469703435897827 lr: [0.01] l1-dist 0.03117002546787262\n",
      "Classifier prediction: -0.5077534317970276\n",
      "2 loss: 1.8804765939712524 lr: [0.009931100837462445] l1-dist 0.037272319197654724\n",
      "Classifier prediction: -0.0021256543695926666\n",
      "4 loss: 1.3650075197219849 lr: [0.009774869058090914] l1-dist 0.03628818690776825\n",
      "Classifier prediction: 0.5076101422309875\n",
      "6 loss: 0.827212929725647 lr: [0.009543642776065642] l1-dist 0.03348230570554733\n",
      "Classifier prediction: 0.9544738531112671\n",
      "8 loss: 0.42429858446121216 lr: [0.009241066670644704] l1-dist 0.037877243012189865\n",
      "Diffusion Counterfactual generated with loss: 0.3994179368019104 | classifier_prediction: 1.0201618671417236 | l1_dist: 0.037925608456134796 | in 10 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:37<00:57, 19.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 48.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -0.4281437397003174\n",
      "0 loss: 1.8882312774658203 lr: [0.01] l1-dist 0.046008750796318054\n",
      "Classifier prediction: 1.6501466035842896\n",
      "2 loss: 1.2291252613067627 lr: [0.009931100837462445] l1-dist 0.05789787322282791\n",
      "Diffusion Counterfactual generated with loss: 1.2291252613067627 | classifier_prediction: 1.6501466035842896 | l1_dist: 0.05789787322282791 | in 3 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:46<00:28, 14.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 39.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -0.27378711104393005\n",
      "0 loss: 1.6039998531341553 lr: [0.01] l1-dist 0.033021267503499985\n",
      "Classifier prediction: -0.028389088809490204\n",
      "2 loss: 1.3078391551971436 lr: [0.009931100837462445] l1-dist 0.027945000678300858\n",
      "Classifier prediction: 0.2643943727016449\n",
      "4 loss: 1.0071477890014648 lr: [0.009774869058090914] l1-dist 0.02715422213077545\n",
      "Classifier prediction: 0.5088465213775635\n",
      "6 loss: 0.7881930470466614 lr: [0.009543642776065642] l1-dist 0.029703956097364426\n",
      "Classifier prediction: 0.6860155463218689\n",
      "8 loss: 0.6025518178939819 lr: [0.009241066670644704] l1-dist 0.028856739401817322\n",
      "Classifier prediction: 0.9097968935966492\n",
      "10 loss: 0.3756050169467926 lr: [0.008871910576983217] l1-dist 0.028540190309286118\n",
      "Classifier prediction: 1.06174635887146\n",
      "12 loss: 0.36188969016075134 lr: [0.008441994226264132] l1-dist 0.030014334246516228\n",
      "Diffusion Counterfactual generated with loss: 0.36188969016075134 | classifier_prediction: 1.06174635887146 | l1_dist: 0.030014334246516228 | in 13 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [01:13<00:19, 19.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 47.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.5056207180023193\n",
      "0 loss: 2.88303279876709 lr: [0.01] l1-dist 0.03774121031165123\n",
      "Classifier prediction: 1.4818617105484009\n",
      "2 loss: 1.1331151723861694 lr: [0.009931100837462445] l1-dist 0.06512534618377686\n",
      "Diffusion Counterfactual generated with loss: 1.1331151723861694 | classifier_prediction: 1.4818617105484009 | l1_dist: 0.06512534618377686 | in 3 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:21<00:00, 16.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import PIL.Image\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import imageio\n",
    "import math\n",
    "import os\n",
    "\n",
    "from diffusers import UNet2DModel, DDIMScheduler, VQModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from zennit.composites import LayerMapComposite\n",
    "from zennit.rules import Epsilon, ZPlus, Pass, Norm\n",
    "\n",
    "from data.dataset import ImageDataset, CelebHQAttrDataset\n",
    "from init_classifier import LinearClassifier, VQVAEClassifier, ResNet50Classifier\n",
    "from xai_lrp import xai_zennit, show_attributions\n",
    "\n",
    "\n",
    "class CheckpointedUNetWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(CheckpointedUNetWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def checkpointed_forward(self, module, *inputs):\n",
    "        def custom_forward(*inputs):\n",
    "            return module(*inputs)\n",
    "        return checkpoint(custom_forward, *inputs)\n",
    "\n",
    "    def forward(self, sample, timestep):\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps * torch.ones(sample.shape[0], dtype=timesteps.dtype, device=timesteps.device)\n",
    "\n",
    "        t_emb = self.model.time_proj(timesteps)\n",
    "        #t_emb = t_emb.to(dtype=self.dtype)\n",
    "        emb = self.model.time_embedding(t_emb)\n",
    "\n",
    "        # 2. pre-process\n",
    "        skip_sample = sample\n",
    "        sample = self.model.conv_in(sample)\n",
    "\n",
    "        # 3. down\n",
    "        down_block_res_samples = (sample,)\n",
    "        for downsample_block in self.model.down_blocks:\n",
    "            if hasattr(downsample_block, \"skip_conv\"):\n",
    "                sample, res_samples, skip_sample = self.checkpointed_forward(downsample_block, sample, emb, skip_sample)\n",
    "            else:\n",
    "                sample, res_samples = self.checkpointed_forward(downsample_block, sample, emb)\n",
    "\n",
    "            down_block_res_samples += res_samples\n",
    "        \n",
    "        # 4. mid\n",
    "        sample = self.checkpointed_forward(self.model.mid_block, sample, emb)\n",
    "\n",
    "        # 5. up\n",
    "        skip_sample = None\n",
    "        for upsample_block in self.model.up_blocks:\n",
    "            res_samples = down_block_res_samples[-len(upsample_block.resnets) :]\n",
    "            down_block_res_samples = down_block_res_samples[: -len(upsample_block.resnets)]\n",
    "\n",
    "            if hasattr(upsample_block, \"skip_conv\"):\n",
    "                sample, skip_sample = self.checkpointed_forward(upsample_block, sample, res_samples, emb, skip_sample)\n",
    "            else:\n",
    "                sample = self.checkpointed_forward(upsample_block, sample, res_samples, emb)\n",
    "\n",
    "        # 6. post-process\n",
    "        sample = self.model.conv_norm_out(sample)\n",
    "        sample = self.model.conv_act(sample)\n",
    "        sample = self.model.conv_out(sample)\n",
    "\n",
    "        if skip_sample is not None:\n",
    "            sample += skip_sample\n",
    "\n",
    "        return {\"sample\": sample}\n",
    "\n",
    "def classifier_loss(classifier, images, targets, idx):\n",
    "    preds = classifier(images)\n",
    "    if idx % 2 == 0:\n",
    "        print(f\"Classifier prediction: {preds[0][cls_id]}\")\n",
    "    targets = torch.tensor(targets).to(device)\n",
    "    #error = torch.nn.functional.binary_cross_entropy_with_logits(preds[0][31], targets)\n",
    "    error = torch.abs(preds[0][cls_id] - targets).mean()\n",
    "    preds_binary = torch.sigmoid(preds[0][cls_id]) > 0.5    \n",
    "\n",
    "    return error, preds_binary\n",
    "\n",
    "def minDist_loss(images, original_images):\n",
    "    error = torch.abs(images - original_images).mean()\n",
    "    return error\n",
    "\n",
    "\n",
    "# data loading with ground truth no smiling\n",
    "data = ImageDataset('/home/dai/GPU-Student-2/Cederic/DataSciPro/data/misclsData_gt0', image_size=256, exts=['jpg', 'JPG', 'png'], do_augment=False, sort_names=True)\n",
    "dataloader = DataLoader(data, batch_size=1, shuffle=False)\n",
    "\n",
    "# create output folders\n",
    "directory_names = []\n",
    "for i, _ in enumerate(dataloader):\n",
    "    img_index = dataloader.dataset.paths[i].name.split('_')[0]\n",
    "    directory_name = os.path.join(\"/home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_ResNet50\", f'folder_IMG_{img_index}')\n",
    "    directory_names.append(directory_name)\n",
    "    os.makedirs(directory_name, exist_ok=True)\n",
    "    print(f'Created directory: {directory_name}')\n",
    "\n",
    "#\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cls_type = 'res50'\n",
    "cls_id =  CelebHQAttrDataset.cls_to_id['Smiling']\n",
    "\n",
    "# load all models\n",
    "unet = UNet2DModel.from_pretrained(\"CompVis/ldm-celebahq-256\", subfolder=\"unet\")\n",
    "vqvae = VQModel.from_pretrained(\"CompVis/ldm-celebahq-256\", subfolder=\"vqvae\")\n",
    "scheduler = DDIMScheduler.from_config(\"CompVis/ldm-celebahq-256\", subfolder=\"scheduler\")\n",
    "\n",
    "unet.to(device)\n",
    "vqvae.to(device)\n",
    "\n",
    "checkpointed_unet = CheckpointedUNetWrapper(unet)\n",
    "\n",
    "# load all models\n",
    "if cls_type == 'linear':    \n",
    "    classifier = LinearClassifier.load_from_checkpoint(\"/home/dai/GPU-Student-2/Cederic/DataSciPro/cls_checkpoints/ffhq256.b128linear2024-06-02 13:08:28.ckpt\",\n",
    "                                            input_dim = data[0]['img'].shape,\n",
    "                                            num_classes = len(CelebHQAttrDataset.id_to_cls))\n",
    "elif cls_type == 'vqvae':\n",
    "    classifier = VQVAEClassifier.load_from_checkpoint(\"/home/dai/GPU-Student-2/Cederic/DataSciPro/cls_checkpoints/ffhq256.b32vqvae2024-06-01 08:48:59.ckpt\",\n",
    "                                           num_classes = len(CelebHQAttrDataset.id_to_cls))\n",
    "    \n",
    "elif cls_type == 'res50':\n",
    "    classifier = ResNet50Classifier.load_from_checkpoint(\"/home/dai/GPU-Student-2/Cederic/DataSciPro/cls_checkpoints/ffhq256.b64res502024-06-02 17:06:41.ckpt\",\n",
    "                                            num_classes = len(CelebHQAttrDataset.id_to_cls))\n",
    "\n",
    "classifier.to(device)\n",
    "classifier.eval()\n",
    "# check functionality of classifier\n",
    "all_outputs = []\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['img'].to(classifier.device)\n",
    "        outputs = classifier(inputs)\n",
    "        print(outputs[0][cls_id])\n",
    "\n",
    "        preds_binary = torch.sigmoid(outputs[:, cls_id].cpu()) > 0.5\n",
    "        all_outputs.append(preds_binary) \n",
    "all_outputs = torch.cat(all_outputs, dim=0)\n",
    "print(all_outputs)\n",
    "\n",
    "\n",
    "###### explainable ai lrp\n",
    "# lrp rules\n",
    "layer_map_lrp_0 = [\n",
    "    (torch.nn.ReLU, Pass()),  # ignore activations\n",
    "    (torch.nn.Linear, Epsilon(epsilon=0)),  # this is the dense Linear, not any Linear\n",
    "    (torch.nn.Conv2d, ZPlus()),\n",
    "    (torch.nn.BatchNorm2d, Pass()),\n",
    "    (torch.nn.AdaptiveAvgPool2d, Norm()),\n",
    "]\n",
    "\n",
    "layer_map_lrp_zplus = [\n",
    "    (torch.nn.ReLU, Pass()),\n",
    "    (torch.nn.Linear, ZPlus()),  # this is the dense Linear, not any Linear\n",
    "    (torch.nn.Conv2d, ZPlus()),\n",
    "    (torch.nn.BatchNorm2d, Pass()),\n",
    "    (torch.nn.AdaptiveAvgPool2d, Norm()),\n",
    "]\n",
    "\n",
    "layer_map_lrp_eps = [\n",
    "    (torch.nn.ReLU, Pass()),\n",
    "    (torch.nn.Linear, Epsilon(epsilon=1)),  # this is the dense Linear, not any Linear\n",
    "    (torch.nn.Conv2d, ZPlus()),\n",
    "    (torch.nn.BatchNorm2d, Pass()),\n",
    "    (torch.nn.AdaptiveAvgPool2d, Norm()),\n",
    "]\n",
    "\n",
    "#before manipulation\n",
    "for i, batch in enumerate(dataloader):\n",
    "    inputs = batch['img'].to(classifier.device)\n",
    "    attr_znt_0 = [xai_zennit(classifier, inputs, RuleComposite=LayerMapComposite(layer_map_lrp_0), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    attr_znt_eps = [xai_zennit(classifier, inputs, RuleComposite=LayerMapComposite(layer_map_lrp_eps), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    attr_znt_zplus = [xai_zennit(classifier, inputs, RuleComposite=LayerMapComposite(layer_map_lrp_zplus), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    show_attributions(directory_names[i], attr_znt_0, title='Pre Zennit LRP-0')\n",
    "    show_attributions(directory_names[i], attr_znt_eps, title='Pre Zennit LRP-EPS')\n",
    "    show_attributions(directory_names[i], attr_znt_zplus, title='Pre Zennit LRP-Z+')\n",
    "        \n",
    "## Inversion\n",
    "def invert(\n",
    "    start_latents,\n",
    "    num_inference_steps,\n",
    "    device=device,\n",
    "):\n",
    "\n",
    "    # Latents are now the specified start latents\n",
    "    latents = start_latents.clone()\n",
    "\n",
    "    # We'll keep a list of the inverted latents as the process goes on\n",
    "    intermediate_latents = []\n",
    "\n",
    "    # Set num inference steps\n",
    "    scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "\n",
    "    # Reversed timesteps <<<<<<<<<<<<<<<<<<<<\n",
    "    timesteps = reversed(scheduler.timesteps)\n",
    "\n",
    "    for i in tqdm(range(1, num_inference_steps), total=num_inference_steps - 1):\n",
    "\n",
    "        # We'll skip the final iteration\n",
    "        if i >= num_inference_steps - 1:\n",
    "            continue\n",
    "\n",
    "        t = timesteps[i]\n",
    "\n",
    "        # Expand the latents if we are doing classifier free guidance\n",
    "        latent_model_input = latents\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "        # Predict the noise residual\n",
    "        noise_pred = checkpointed_unet(latent_model_input, t)[\"sample\"]\n",
    "\n",
    "        current_t = max(0, t.item() - (1000 // num_inference_steps))  # t\n",
    "        next_t = t  # min(999, t.item() + (1000//num_inference_steps)) # t+1\n",
    "        alpha_t = scheduler.alphas_cumprod[current_t]\n",
    "        alpha_t_next = scheduler.alphas_cumprod[next_t]\n",
    "\n",
    "        # Inverted update step (re-arranging the update step to get x(t) (new latents) as a function of x(t-1) (current latents)\n",
    "        latents = (latents - (1 - alpha_t).sqrt() * noise_pred) * (alpha_t_next.sqrt() / alpha_t.sqrt()) + (\n",
    "            1 - alpha_t_next\n",
    "        ).sqrt() * noise_pred\n",
    "\n",
    "        # Store\n",
    "        intermediate_latents.append(latents)\n",
    "\n",
    "    return torch.cat(intermediate_latents)\n",
    "\n",
    "\n",
    "class LatentNoise(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The LatentNoise Module makes it easier to update the noise tensor with torch optimizers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, noise: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.noise = torch.nn.Parameter(noise)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.noise\n",
    "\n",
    "\n",
    "def diffusion_pipe(noise_module: LatentNoise, num_inference_steps):\n",
    "        z = noise_module()\n",
    "        for i in range(start_step, num_inference_steps):\n",
    "            t = scheduler.timesteps[i]\n",
    "            z = scheduler.scale_model_input(z, t)\n",
    "            with torch.no_grad():\n",
    "                noise_pred = checkpointed_unet(z, t)[\"sample\"]\n",
    "            z = scheduler.step(noise_pred, t, z).prev_sample\n",
    "            z0 = scheduler.step(noise_pred, t, z).pred_original_sample\n",
    "        return z, z0\n",
    "\n",
    "def plot_images(images, titles=None, figsize=(50, 5), save_path=None):\n",
    "    n = len(images)\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "        if titles is not None:\n",
    "            axes[i].set_title(titles[i])\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    #plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_to_pil(tensor):\n",
    "    image = tensor.cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    image = PIL.Image.fromarray(np.array(image[0].detach().numpy() * 255).astype(np.uint8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def tensor_to_pil_image(tensor):\n",
    "    image = tensor.cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    image = PIL.Image.fromarray(np.array(image[0].detach().numpy() * 255).astype(np.uint8))\n",
    "    return image\n",
    "\n",
    "# conditional sampling\n",
    "num_inference_steps = 100\n",
    "start_step = 20\n",
    "\n",
    "\n",
    "for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "    #plot_to_pil(batch['img'])\n",
    "    with torch.no_grad():\n",
    "        z = vqvae.encode(batch['img'].to(device))   # encode the image in the latent space\n",
    "    z = z.latents\n",
    "    \n",
    "    #cond = z.view(1,-1)\n",
    "    #cond = normalize(cond)\n",
    "    #cond = cond + 0.5 * math.sqrt(512) * classifier.fc1.weight[31].unsqueeze(0)\n",
    "    #cond = denormalize(cond)\n",
    "    #z = cond.view(1,3,64,64)\n",
    "    #dec_z = vqvae.decode(z)[0]\n",
    "    #plot_to_pil(dec_z)\n",
    "    \n",
    "    inverted_latents = invert(z, num_inference_steps)                  # do the ddim scheduler reversed to add noise to the latents\n",
    "    z = inverted_latents[-(start_step + 1)].unsqueeze(0)                  # use these latents to start the sampling. better performance when using not the last latent sample\n",
    "    noise_module = LatentNoise(z.clone()).to(device)                    # convert latent noise to a parameter module for optimization\n",
    "    noise_module.noise.requires_grad = True\n",
    "    intermediate_results = [batch['img'].to(device)]   # list to store the results of the steering\n",
    "    intermediate_preds = [round(classifier(batch['img'].to(device))[0][cls_id].item(), 5)]\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        noise_module.parameters(), lr=0.01, maximize=False # not minimize gradient ascent\n",
    "    )\n",
    "    learning_scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "    \n",
    "    x = torch.zeros_like(z)\n",
    "    current_loss = float('inf')\n",
    "    preds_binary = False\n",
    "    current_pred = 0.0\n",
    "    i = 0\n",
    "    while (current_pred < 1.0) & (i < 20) :\n",
    "    #for i in tqdm(range(num_optimization_steps)):\n",
    "            optimizer.zero_grad()\n",
    "            x, x0 = diffusion_pipe(noise_module, num_inference_steps) # forward\n",
    "            decoded_x = vqvae.decode(x)[0]\n",
    "            current_pred = classifier(decoded_x)[0][cls_id]\n",
    "\n",
    "            if i % 1 == 0:\n",
    "                intermediate_results.append(decoded_x)\n",
    "                intermediate_preds.append(round(current_pred.item(), 5))\n",
    "\n",
    "            loss, preds_binary = classifier_loss(classifier, decoded_x, 1.0, i)\n",
    "            l1_dist = minDist_loss(decoded_x, batch['img'].to(device))\n",
    "            loss += l1_dist * 10\n",
    "            \n",
    "            if i % 2 == 0:\n",
    "                print(i, \"loss:\", loss.item(), \"lr:\", learning_scheduler.get_lr(), \"l1-dist\", l1_dist.item())\n",
    "                #print(i, \"loss:\", loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            learning_scheduler.step()\n",
    "\n",
    "            current_loss = loss.item()\n",
    "            i += 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image = vqvae.decode(x)[0]\n",
    "\n",
    "    print(f\"Diffusion Counterfactual generated with loss: {current_loss} | classifier_prediction: {current_pred} | l1_dist: {l1_dist} | in {i} optimization steps\")\n",
    "    \n",
    "    #lrp after manipulation\n",
    "    attr_znt_0 = [xai_zennit(classifier, image, RuleComposite=LayerMapComposite(layer_map_lrp_0), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    attr_znt_eps = [xai_zennit(classifier, image, RuleComposite=LayerMapComposite(layer_map_lrp_eps), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    attr_znt_zplus = [xai_zennit(classifier, image, RuleComposite=LayerMapComposite(layer_map_lrp_zplus), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    show_attributions(directory_names[step], attr_znt_0, title='Post Zennit LRP-0')\n",
    "    show_attributions(directory_names[step], attr_znt_eps, title='Post Zennit LRP-EPS')\n",
    "    show_attributions(directory_names[step], attr_znt_zplus, title='Post Zennit LRP-Z+')\n",
    "    image.requires_grad = False\n",
    "    \n",
    "    images = [tensor_to_pil_image(tensor) for tensor in intermediate_results]\n",
    "    gif_path = f\"{directory_names[step]}/GIF.gif\"\n",
    "    imageio.mimsave(gif_path, images, format='GIF', duration=2.0, loop=0)  # duration is in seconds\n",
    "\n",
    "    row_path = f\"{directory_names[step]}/sequence.png\"\n",
    "    plot_images(images, intermediate_preds, save_path=row_path)\n",
    "\n",
    "    # process image\n",
    "    image_processed = image.cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    image_pil = PIL.Image.fromarray(np.array(image_processed[0] * 255).astype(np.uint8))\n",
    "    ori_processed = batch['img'].cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    ori_image = PIL.Image.fromarray(np.array(ori_processed[0] * 255).astype(np.uint8))\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(image_pil)\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title('Diffusion Counterfactual Image')\n",
    "    axs[1].imshow(ori_image)\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title('Original Image')\n",
    "    #plt.show()\n",
    "    fig.savefig(f'{directory_names[step]}/ori_vs_DCE.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    image_pil.save(f\"{directory_names[step]}/diffCounter_IMG.png\")\n",
    "    print('finish')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffcounter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
