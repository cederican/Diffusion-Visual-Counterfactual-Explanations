{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_Linear/sequence/folder_IMG_27300\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_Linear/sequence/folder_IMG_27398\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_Linear/sequence/folder_IMG_27591\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_Linear/sequence/folder_IMG_27611\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_Linear/sequence/folder_IMG_27931\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_Linear/sequence/folder_IMG_28113\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_Linear/sequence/folder_IMG_28125\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_Linear/sequence/folder_IMG_28285\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_Linear/sequence/folder_IMG_28354\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_Linear/sequence/folder_IMG_28355\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_Linear/sequence/folder_IMG_28362\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_Linear/sequence/folder_IMG_28383\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_Linear/sequence/folder_IMG_28583\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_Linear/sequence/folder_IMG_28670\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_Linear/sequence/folder_IMG_28782\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_Linear/sequence/folder_IMG_28892\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_Linear/sequence/folder_IMG_29058\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_Linear/sequence/folder_IMG_29188\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_Linear/sequence/folder_IMG_29408\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_Linear/sequence/folder_IMG_29527\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_Linear/sequence/folder_IMG_29762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dai/GPU-Student-2/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/dai/GPU-Student-2/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddim.DDIMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "The config attributes {'timestep_values': None, 'timesteps': 1000} were passed to DDIMScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.2704, device='cuda:0')\n",
      "tensor(-1.6096, device='cuda:0')\n",
      "tensor(-2.0562, device='cuda:0')\n",
      "tensor(-1.1663, device='cuda:0')\n",
      "tensor(-1.7197, device='cuda:0')\n",
      "tensor(-0.7761, device='cuda:0')\n",
      "tensor(-1.8571, device='cuda:0')\n",
      "tensor(-0.5018, device='cuda:0')\n",
      "tensor(-1.8218, device='cuda:0')\n",
      "tensor(-2.5475, device='cuda:0')\n",
      "tensor(-1.2291, device='cuda:0')\n",
      "tensor(-1.5236, device='cuda:0')\n",
      "tensor(-2.1163, device='cuda:0')\n",
      "tensor(-1.3400, device='cuda:0')\n",
      "tensor(-1.9986, device='cuda:0')\n",
      "tensor(-1.7572, device='cuda:0')\n",
      "tensor(-1.3951, device='cuda:0')\n",
      "tensor(-1.2508, device='cuda:0')\n",
      "tensor(-1.5588, device='cuda:0')\n",
      "tensor(-1.4129, device='cuda:0')\n",
      "tensor(-0.3220, device='cuda:0')\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]/home/dai/GPU-Student-2/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "100%|██████████| 99/99 [00:02<00:00, 40.09it/s]\n",
      "/home/dai/GPU-Student-2/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.37044358253479\n",
      "0 loss: 2.778909921646118 lr: [0.01] l1-dist 0.040846627205610275\n",
      "Classifier prediction: 0.2841572165489197\n",
      "2 loss: 1.4180927276611328 lr: [0.009931100837462445] l1-dist 0.07022498548030853\n",
      "Classifier prediction: 1.2118208408355713\n",
      "4 loss: 1.0750572681427002 lr: [0.009774869058090914] l1-dist 0.08632364124059677\n",
      "Diffusion Counterfactual generated with loss: 1.0750572681427002 | classifier_prediction: 1.2118208408355713 | l1_dist: 0.08632364124059677 | in 5 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1/21 [00:55<18:21, 55.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dai/GPU-Student-2/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "100%|██████████| 99/99 [00:02<00:00, 40.86it/s]\n",
      "/home/dai/GPU-Student-2/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.7357804775238037\n",
      "0 loss: 3.1483583450317383 lr: [0.01] l1-dist 0.04125778749585152\n",
      "Classifier prediction: 0.14300379157066345\n",
      "2 loss: 1.7048193216323853 lr: [0.009931100837462445] l1-dist 0.08478231728076935\n",
      "Classifier prediction: 1.5644819736480713\n",
      "4 loss: 1.6893174648284912 lr: [0.009774869058090914] l1-dist 0.11248354613780975\n",
      "Diffusion Counterfactual generated with loss: 1.6893174648284912 | classifier_prediction: 1.5644819736480713 | l1_dist: 0.11248354613780975 | in 5 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 2/21 [01:07<09:33, 30.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 41.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -2.0542335510253906\n",
      "0 loss: 3.383463144302368 lr: [0.01] l1-dist 0.03292296826839447\n",
      "Classifier prediction: -1.362311840057373\n",
      "2 loss: 3.008636474609375 lr: [0.009931100837462445] l1-dist 0.06463246047496796\n",
      "Classifier prediction: 1.2724076509475708\n",
      "4 loss: 1.078696846961975 lr: [0.009774869058090914] l1-dist 0.08062891662120819\n",
      "Diffusion Counterfactual generated with loss: 1.078696846961975 | classifier_prediction: 1.2724076509475708 | l1_dist: 0.08062891662120819 | in 5 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 3/21 [01:20<06:39, 22.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 41.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.245290756225586\n",
      "0 loss: 2.685704469680786 lr: [0.01] l1-dist 0.04404136538505554\n",
      "Classifier prediction: -0.2049323320388794\n",
      "2 loss: 1.78743577003479 lr: [0.009931100837462445] l1-dist 0.05825034901499748\n",
      "Classifier prediction: 0.19136324524879456\n",
      "4 loss: 1.507222294807434 lr: [0.009774869058090914] l1-dist 0.06985855102539062\n",
      "Classifier prediction: 0.5631318092346191\n",
      "6 loss: 1.2534486055374146 lr: [0.009543642776065642] l1-dist 0.08165804296731949\n",
      "Classifier prediction: 1.0101597309112549\n",
      "8 loss: 0.9846431612968445 lr: [0.009241066670644704] l1-dist 0.09744834154844284\n",
      "Diffusion Counterfactual generated with loss: 0.9846431612968445 | classifier_prediction: 1.0101597309112549 | l1_dist: 0.09744834154844284 | in 9 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 4/21 [01:41<06:08, 21.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 41.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.8131568431854248\n",
      "0 loss: 3.146090269088745 lr: [0.01] l1-dist 0.03329335153102875\n",
      "Classifier prediction: 0.5467492938041687\n",
      "2 loss: 0.9504333138465881 lr: [0.009931100837462445] l1-dist 0.049718260765075684\n",
      "Diffusion Counterfactual generated with loss: 0.8481041789054871 | classifier_prediction: 1.237642526626587 | l1_dist: 0.061046164482831955 | in 4 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 5/21 [01:52<04:45, 17.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 41.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -0.7897571325302124\n",
      "0 loss: 2.173189640045166 lr: [0.01] l1-dist 0.038343243300914764\n",
      "Classifier prediction: 1.167104721069336\n",
      "2 loss: 0.9912639856338501 lr: [0.009931100837462445] l1-dist 0.08241592347621918\n",
      "Diffusion Counterfactual generated with loss: 0.9912639856338501 | classifier_prediction: 1.167104721069336 | l1_dist: 0.08241592347621918 | in 3 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 6/21 [02:01<03:40, 14.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 41.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.9079196453094482\n",
      "0 loss: 3.2196199893951416 lr: [0.01] l1-dist 0.03117002546787262\n",
      "Classifier prediction: -1.2846229076385498\n",
      "2 loss: 2.700162649154663 lr: [0.009931100837462445] l1-dist 0.041553981602191925\n",
      "Classifier prediction: -0.7463563680648804\n",
      "4 loss: 2.270904541015625 lr: [0.009774869058090914] l1-dist 0.05245480686426163\n",
      "Classifier prediction: 0.8227582573890686\n",
      "6 loss: 0.8845946192741394 lr: [0.009543642776065642] l1-dist 0.07073529064655304\n",
      "Diffusion Counterfactual generated with loss: 0.9941493272781372 | classifier_prediction: 1.2195571660995483 | l1_dist: 0.07745921611785889 | in 8 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 7/21 [02:20<03:46, 16.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 40.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -0.48073992133140564\n",
      "0 loss: 1.940827488899231 lr: [0.01] l1-dist 0.046008750796318054\n",
      "Classifier prediction: 1.7068097591400146\n",
      "2 loss: 1.3464093208312988 lr: [0.009931100837462445] l1-dist 0.06395995616912842\n",
      "Diffusion Counterfactual generated with loss: 1.3464093208312988 | classifier_prediction: 1.7068097591400146 | l1_dist: 0.06395995616912842 | in 3 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 8/21 [02:29<02:59, 13.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 40.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.9188576936721802\n",
      "0 loss: 3.249070167541504 lr: [0.01] l1-dist 0.033021267503499985\n",
      "Classifier prediction: -1.4899877309799194\n",
      "2 loss: 2.883683204650879 lr: [0.009931100837462445] l1-dist 0.03936952352523804\n",
      "Classifier prediction: -1.0859318971633911\n",
      "4 loss: 2.5724844932556152 lr: [0.009774869058090914] l1-dist 0.04865526407957077\n",
      "Classifier prediction: -0.703542947769165\n",
      "6 loss: 2.207361936569214 lr: [0.009543642776065642] l1-dist 0.05038190633058548\n",
      "Classifier prediction: -0.26784756779670715\n",
      "8 loss: 1.7843916416168213 lr: [0.009241066670644704] l1-dist 0.051654405891895294\n",
      "Classifier prediction: 0.20653674006462097\n",
      "10 loss: 1.3570643663406372 lr: [0.008871910576983217] l1-dist 0.05636011064052582\n",
      "Classifier prediction: 0.6826080679893494\n",
      "12 loss: 0.9454736113548279 lr: [0.008441994226264132] l1-dist 0.06280817091464996\n",
      "Classifier prediction: 1.227134346961975\n",
      "14 loss: 0.9569183588027954 lr: [0.007958095421518055] l1-dist 0.07297839969396591\n",
      "Diffusion Counterfactual generated with loss: 0.9569183588027954 | classifier_prediction: 1.227134346961975 | l1_dist: 0.07297839969396591 | in 15 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 9/21 [03:02<03:59, 19.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 40.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -2.676177501678467\n",
      "0 loss: 4.092867374420166 lr: [0.01] l1-dist 0.041669003665447235\n",
      "Classifier prediction: 1.6016203165054321\n",
      "2 loss: 1.6480499505996704 lr: [0.009931100837462445] l1-dist 0.10464295744895935\n",
      "Diffusion Counterfactual generated with loss: 1.6480499505996704 | classifier_prediction: 1.6016203165054321 | l1_dist: 0.10464295744895935 | in 3 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 10/21 [03:11<03:01, 16.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 35.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.2535080909729004\n",
      "0 loss: 2.513293504714966 lr: [0.01] l1-dist 0.025978539139032364\n",
      "Classifier prediction: -0.19536444544792175\n",
      "2 loss: 1.964280128479004 lr: [0.009931100837462445] l1-dist 0.07689157128334045\n",
      "Classifier prediction: 0.3826928734779358\n",
      "4 loss: 1.5920321941375732 lr: [0.009774869058090914] l1-dist 0.09747251123189926\n",
      "Classifier prediction: 1.0411041975021362\n",
      "6 loss: 1.0880900621414185 lr: [0.009543642776065642] l1-dist 0.10469858348369598\n",
      "Diffusion Counterfactual generated with loss: 1.0880900621414185 | classifier_prediction: 1.0411041975021362 | l1_dist: 0.10469858348369598 | in 7 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 11/21 [03:28<02:48, 16.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 40.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.5595449209213257\n",
      "0 loss: 3.143853187561035 lr: [0.01] l1-dist 0.05843082070350647\n",
      "Classifier prediction: 0.9460344314575195\n",
      "2 loss: 0.8956513404846191 lr: [0.009931100837462445] l1-dist 0.08416857570409775\n",
      "Diffusion Counterfactual generated with loss: 1.178856372833252 | classifier_prediction: 1.2044706344604492 | l1_dist: 0.09743857383728027 | in 4 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 12/21 [03:39<02:14, 15.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 40.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -2.242992401123047\n",
      "0 loss: 3.6384997367858887 lr: [0.01] l1-dist 0.03955074027180672\n",
      "Classifier prediction: 0.31294694542884827\n",
      "2 loss: 1.325820803642273 lr: [0.009931100837462445] l1-dist 0.06387677043676376\n",
      "Classifier prediction: 0.36876270174980164\n",
      "4 loss: 1.3447773456573486 lr: [0.009774869058090914] l1-dist 0.07135400176048279\n",
      "Classifier prediction: 1.6399176120758057\n",
      "6 loss: 2.0076284408569336 lr: [0.009543642776065642] l1-dist 0.1367710828781128\n",
      "Diffusion Counterfactual generated with loss: 2.0076284408569336 | classifier_prediction: 1.6399176120758057 | l1_dist: 0.1367710828781128 | in 7 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 13/21 [03:56<02:04, 15.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 40.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.5507910251617432\n",
      "0 loss: 3.0091726779937744 lr: [0.01] l1-dist 0.04583815857768059\n",
      "Diffusion Counterfactual generated with loss: 1.088670253753662 | classifier_prediction: 1.3349753618240356 | l1_dist: 0.07536948472261429 | in 2 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 14/21 [04:03<01:30, 12.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 40.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -2.0247175693511963\n",
      "0 loss: 3.405848503112793 lr: [0.01] l1-dist 0.03811308741569519\n",
      "Classifier prediction: -1.374620795249939\n",
      "2 loss: 2.978684663772583 lr: [0.009931100837462445] l1-dist 0.06040636822581291\n",
      "Classifier prediction: -0.5683680176734924\n",
      "4 loss: 2.334089994430542 lr: [0.009774869058090914] l1-dist 0.07657220959663391\n",
      "Classifier prediction: 1.0921351909637451\n",
      "6 loss: 1.0457799434661865 lr: [0.009543642776065642] l1-dist 0.09536447376012802\n",
      "Diffusion Counterfactual generated with loss: 1.0457799434661865 | classifier_prediction: 1.0921351909637451 | l1_dist: 0.09536447376012802 | in 7 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 15/21 [04:20<01:24, 14.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 40.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -2.0431790351867676\n",
      "0 loss: 3.5878217220306396 lr: [0.01] l1-dist 0.054464273154735565\n",
      "Classifier prediction: -0.8416629433631897\n",
      "2 loss: 2.567584991455078 lr: [0.009931100837462445] l1-dist 0.07259220629930496\n",
      "Classifier prediction: 0.04257919639348984\n",
      "4 loss: 1.9767436981201172 lr: [0.009774869058090914] l1-dist 0.10193228721618652\n",
      "Classifier prediction: 1.2578797340393066\n",
      "6 loss: 1.5982271432876587 lr: [0.009543642776065642] l1-dist 0.13403473794460297\n",
      "Diffusion Counterfactual generated with loss: 1.5982271432876587 | classifier_prediction: 1.2578797340393066 | l1_dist: 0.13403473794460297 | in 7 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 16/21 [04:37<01:15, 15.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 40.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.441793441772461\n",
      "0 loss: 2.925058364868164 lr: [0.01] l1-dist 0.048326484858989716\n",
      "Classifier prediction: -0.20588275790214539\n",
      "2 loss: 1.9482216835021973 lr: [0.009931100837462445] l1-dist 0.07423388212919235\n",
      "Classifier prediction: 1.1445190906524658\n",
      "4 loss: 1.0179715156555176 lr: [0.009774869058090914] l1-dist 0.08734524250030518\n",
      "Diffusion Counterfactual generated with loss: 1.0179715156555176 | classifier_prediction: 1.1445190906524658 | l1_dist: 0.08734524250030518 | in 5 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 17/21 [04:50<00:57, 14.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 40.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.2903563976287842\n",
      "0 loss: 2.64945912361145 lr: [0.01] l1-dist 0.035910263657569885\n",
      "Classifier prediction: -0.279375284910202\n",
      "2 loss: 1.8135093450546265 lr: [0.009931100837462445] l1-dist 0.053413406014442444\n",
      "Classifier prediction: 0.44443801045417786\n",
      "4 loss: 1.2426519393920898 lr: [0.009774869058090914] l1-dist 0.06870898604393005\n",
      "Classifier prediction: 1.358891487121582\n",
      "6 loss: 1.2175981998443604 lr: [0.009543642776065642] l1-dist 0.0858706682920456\n",
      "Diffusion Counterfactual generated with loss: 1.2175981998443604 | classifier_prediction: 1.358891487121582 | l1_dist: 0.0858706682920456 | in 7 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 18/21 [05:07<00:45, 15.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 40.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.6596801280975342\n",
      "0 loss: 3.0025620460510254 lr: [0.01] l1-dist 0.034288205206394196\n",
      "Classifier prediction: -1.2291425466537476\n",
      "2 loss: 2.5688636302948 lr: [0.009931100837462445] l1-dist 0.03397208824753761\n",
      "Classifier prediction: -0.3057168424129486\n",
      "4 loss: 1.823319435119629 lr: [0.009774869058090914] l1-dist 0.05176025629043579\n",
      "Diffusion Counterfactual generated with loss: 0.6584835052490234 | classifier_prediction: 1.052412509918213 | l1_dist: 0.060607098042964935 | in 6 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 19/21 [05:22<00:30, 15.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 40.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.351494312286377\n",
      "0 loss: 2.885748863220215 lr: [0.01] l1-dist 0.05342545360326767\n",
      "Classifier prediction: -0.20971660315990448\n",
      "2 loss: 1.9263710975646973 lr: [0.009931100837462445] l1-dist 0.071665458381176\n",
      "Diffusion Counterfactual generated with loss: 1.061234712600708 | classifier_prediction: 1.0635846853256226 | l1_dist: 0.09976499527692795 | in 4 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 20/21 [05:33<00:13, 13.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 40.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -0.3589981496334076\n",
      "0 loss: 1.7364102602005005 lr: [0.01] l1-dist 0.03774121031165123\n",
      "Classifier prediction: 2.9557957649230957\n",
      "2 loss: 2.707279682159424 lr: [0.009931100837462445] l1-dist 0.07514839619398117\n",
      "Diffusion Counterfactual generated with loss: 2.707279682159424 | classifier_prediction: 2.9557957649230957 | l1_dist: 0.07514839619398117 | in 3 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [05:42<00:00, 16.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import PIL.Image\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "from diffusers import UNet2DModel, DDIMScheduler, VQModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from zennit.composites import LayerMapComposite\n",
    "from zennit.rules import Epsilon, ZPlus, Pass, Norm\n",
    "\n",
    "from data.dataset import ImageDataset, CelebHQAttrDataset\n",
    "from init_classifier import LinearClassifier, VQVAEClassifier, ResNet50Classifier, ViTClassifier\n",
    "from xai_lrp import xai_zennit, show_attributions\n",
    "\n",
    "\n",
    "class CheckpointedUNetWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(CheckpointedUNetWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def checkpointed_forward(self, module, *inputs):\n",
    "        def custom_forward(*inputs):\n",
    "            return module(*inputs)\n",
    "        return checkpoint(custom_forward, *inputs)\n",
    "\n",
    "    def forward(self, sample, timestep):\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps * torch.ones(sample.shape[0], dtype=timesteps.dtype, device=timesteps.device)\n",
    "\n",
    "        t_emb = self.model.time_proj(timesteps)\n",
    "        #t_emb = t_emb.to(dtype=self.dtype)\n",
    "        emb = self.model.time_embedding(t_emb)\n",
    "\n",
    "        # 2. pre-process\n",
    "        skip_sample = sample\n",
    "        sample = self.model.conv_in(sample)\n",
    "\n",
    "        # 3. down\n",
    "        down_block_res_samples = (sample,)\n",
    "        for downsample_block in self.model.down_blocks:\n",
    "            if hasattr(downsample_block, \"skip_conv\"):\n",
    "                sample, res_samples, skip_sample = self.checkpointed_forward(downsample_block, sample, emb, skip_sample)\n",
    "            else:\n",
    "                sample, res_samples = self.checkpointed_forward(downsample_block, sample, emb)\n",
    "\n",
    "            down_block_res_samples += res_samples\n",
    "\n",
    "        # 4. mid\n",
    "        sample = self.checkpointed_forward(self.model.mid_block, sample, emb)\n",
    "\n",
    "        # 5. up\n",
    "        skip_sample = None\n",
    "        for upsample_block in self.model.up_blocks:\n",
    "            res_samples = down_block_res_samples[-len(upsample_block.resnets) :]\n",
    "            down_block_res_samples = down_block_res_samples[: -len(upsample_block.resnets)]\n",
    "\n",
    "            if hasattr(upsample_block, \"skip_conv\"):\n",
    "                sample, skip_sample = self.checkpointed_forward(upsample_block, sample, res_samples, emb, skip_sample)\n",
    "            else:\n",
    "                sample = self.checkpointed_forward(upsample_block, sample, res_samples, emb)\n",
    "\n",
    "        # 6. post-process\n",
    "        sample = self.model.conv_norm_out(sample)\n",
    "        sample = self.model.conv_act(sample)\n",
    "        sample = self.model.conv_out(sample)\n",
    "\n",
    "        if skip_sample is not None:\n",
    "            sample += skip_sample\n",
    "\n",
    "        return {\"sample\": sample}\n",
    "\n",
    "def classifier_loss(classifier, images, targets, idx):\n",
    "    preds = classifier(images)\n",
    "    if idx % 2 == 0:\n",
    "        print(f\"Classifier prediction: {preds[0][cls_id]}\")\n",
    "    targets = torch.tensor(targets).to(device)\n",
    "    #error = torch.nn.functional.binary_cross_entropy_with_logits(preds[0][31], targets)\n",
    "    error = torch.abs(preds[0][cls_id] - targets).mean()\n",
    "    preds_binary = torch.sigmoid(preds[0][cls_id]) > 0.5\n",
    "\n",
    "    return error, preds_binary\n",
    "\n",
    "def minDist_loss(images, original_images):\n",
    "    error = torch.abs(images - original_images).mean()\n",
    "    return error\n",
    "\n",
    "\n",
    "# data loading with ground truth no smiling\n",
    "data = ImageDataset('/home/dai/GPU-Student-2/Cederic/DataSciPro/data/misclsData_gt1', image_size=256, exts=['jpg', 'JPG', 'png'], do_augment=False, sort_names=True)\n",
    "dataloader = DataLoader(data, batch_size=1, shuffle=False)\n",
    "\n",
    "# create output folders\n",
    "directory_names = []\n",
    "for i, _ in enumerate(dataloader):\n",
    "    img_index = dataloader.dataset.paths[i].name.split('_')[0]\n",
    "    directory_name = os.path.join(\"/home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_Linear\", f'folder_IMG_{img_index}')\n",
    "    directory_names.append(directory_name)\n",
    "    os.makedirs(directory_name, exist_ok=True)\n",
    "    print(f'Created directory: {directory_name}')\n",
    "\n",
    "#\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cls_type = 'linear'\n",
    "cls_id =  CelebHQAttrDataset.cls_to_id['Smiling']\n",
    "\n",
    "# load all models\n",
    "unet = UNet2DModel.from_pretrained(\"CompVis/ldm-celebahq-256\", subfolder=\"unet\")\n",
    "vqvae = VQModel.from_pretrained(\"CompVis/ldm-celebahq-256\", subfolder=\"vqvae\")\n",
    "scheduler = DDIMScheduler.from_config(\"CompVis/ldm-celebahq-256\", subfolder=\"scheduler\")\n",
    "\n",
    "unet.to(device)\n",
    "vqvae.to(device)\n",
    "\n",
    "checkpointed_unet = CheckpointedUNetWrapper(unet)\n",
    "\n",
    "# load all models\n",
    "if cls_type == 'linear':    \n",
    "    classifier = LinearClassifier.load_from_checkpoint(\"/home/dai/GPU-Student-2/Cederic/DataSciPro/cls_checkpoints/ffhq256.b128linear2024-06-02 13:08:28.ckpt\",\n",
    "                                            input_dim = data[0]['img'].shape,\n",
    "                                            num_classes = len(CelebHQAttrDataset.id_to_cls))\n",
    "elif cls_type == 'vqvae':\n",
    "    classifier = VQVAEClassifier.load_from_checkpoint(\"/home/dai/GPU-Student-2/Cederic/DataSciPro/cls_checkpoints/ffhq256.b32vqvae2024-06-01 08:48:59.ckpt\",\n",
    "                                       num_classes = len(CelebHQAttrDataset.id_to_cls))\n",
    "\n",
    "elif cls_type == 'res50':\n",
    "    classifier = ResNet50Classifier.load_from_checkpoint(\"/home/dai/GPU-Student-2/Cederic/DataSciPro/cls_checkpoints/ffhq256.b64res502024-06-02 17:06:41.ckpt\",\n",
    "                                            num_classes = len(CelebHQAttrDataset.id_to_cls))\n",
    "\n",
    "\n",
    "classifier.to(device)\n",
    "classifier.eval()\n",
    "# check functionality of classifier\n",
    "all_outputs = []\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['img'].to(classifier.device)\n",
    "        outputs = classifier(inputs)\n",
    "        print(outputs[0][cls_id])\n",
    "\n",
    "        preds_binary = torch.sigmoid(outputs[:, cls_id].cpu()) > 0.5\n",
    "        all_outputs.append(preds_binary) \n",
    "all_outputs = torch.cat(all_outputs, dim=0)\n",
    "print(all_outputs)\n",
    "\n",
    "\n",
    "###### explainable ai lrp\n",
    "# lrp rules\n",
    "layer_map_lrp_0 = [\n",
    "    (torch.nn.ReLU, Pass()),  # ignore activations\n",
    "    (torch.nn.Linear, Epsilon(epsilon=0)),  # this is the dense Linear, not any Linear\n",
    "    (torch.nn.Conv2d, ZPlus()),\n",
    "    (torch.nn.BatchNorm2d, Pass()),\n",
    "    (torch.nn.AdaptiveAvgPool2d, Norm()),\n",
    "]\n",
    "\n",
    "layer_map_lrp_zplus = [\n",
    "    (torch.nn.ReLU, Pass()),\n",
    "    (torch.nn.Linear, ZPlus()),  # this is the dense Linear, not any Linear\n",
    "    (torch.nn.Conv2d, ZPlus()),\n",
    "    (torch.nn.BatchNorm2d, Pass()),\n",
    "    (torch.nn.AdaptiveAvgPool2d, Norm()),\n",
    "]\n",
    "\n",
    "layer_map_lrp_eps = [\n",
    "    (torch.nn.ReLU, Pass()),\n",
    "    (torch.nn.Linear, Epsilon(epsilon=1)),  # this is the dense Linear, not any Linear\n",
    "    (torch.nn.Conv2d, ZPlus()),\n",
    "    (torch.nn.BatchNorm2d, Pass()),\n",
    "    (torch.nn.AdaptiveAvgPool2d, Norm()),\n",
    "]\n",
    "\n",
    "#before manipulation\n",
    "for i, batch in enumerate(dataloader):\n",
    "    inputs = batch['img'].to(classifier.device)\n",
    "    attr_znt_0 = [xai_zennit(classifier, inputs, RuleComposite=LayerMapComposite(layer_map_lrp_0), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    attr_znt_eps = [xai_zennit(classifier, inputs, RuleComposite=LayerMapComposite(layer_map_lrp_eps), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    attr_znt_zplus = [xai_zennit(classifier, inputs, RuleComposite=LayerMapComposite(layer_map_lrp_zplus), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    show_attributions(directory_names[i], attr_znt_0, title='Pre Zennit LRP-0')\n",
    "    show_attributions(directory_names[i], attr_znt_eps, title='Pre Zennit LRP-EPS')\n",
    "    show_attributions(directory_names[i], attr_znt_zplus, title='Pre Zennit LRP-Z+')\n",
    "        \n",
    "## Inversion\n",
    "def invert(\n",
    "    start_latents,\n",
    "    num_inference_steps,\n",
    "    device=device,\n",
    "):\n",
    "\n",
    "    # Latents are now the specified start latents\n",
    "    latents = start_latents.clone()\n",
    "\n",
    "    # We'll keep a list of the inverted latents as the process goes on\n",
    "    intermediate_latents = []\n",
    "\n",
    "    # Set num inference steps\n",
    "    scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "\n",
    "    # Reversed timesteps <<<<<<<<<<<<<<<<<<<<\n",
    "    timesteps = reversed(scheduler.timesteps)\n",
    "\n",
    "    for i in tqdm(range(1, num_inference_steps), total=num_inference_steps - 1):\n",
    "\n",
    "        # We'll skip the final iteration\n",
    "        if i >= num_inference_steps - 1:\n",
    "            continue\n",
    "\n",
    "        t = timesteps[i]\n",
    "\n",
    "        # Expand the latents if we are doing classifier free guidance\n",
    "        latent_model_input = latents\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "        # Predict the noise residual\n",
    "        noise_pred = checkpointed_unet(latent_model_input, t)[\"sample\"]\n",
    "\n",
    "        current_t = max(0, t.item() - (1000 // num_inference_steps))  # t\n",
    "        next_t = t  # min(999, t.item() + (1000//num_inference_steps)) # t+1\n",
    "        alpha_t = scheduler.alphas_cumprod[current_t]\n",
    "        alpha_t_next = scheduler.alphas_cumprod[next_t]\n",
    "\n",
    "        # Inverted update step (re-arranging the update step to get x(t) (new latents) as a function of x(t-1) (current latents)\n",
    "        latents = (latents - (1 - alpha_t).sqrt() * noise_pred) * (alpha_t_next.sqrt() / alpha_t.sqrt()) + (\n",
    "            1 - alpha_t_next\n",
    "        ).sqrt() * noise_pred\n",
    "\n",
    "        # Store\n",
    "        intermediate_latents.append(latents)\n",
    "\n",
    "    return torch.cat(intermediate_latents)\n",
    "\n",
    "\n",
    "class LatentNoise(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The LatentNoise Module makes it easier to update the noise tensor with torch optimizers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, noise: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.noise = torch.nn.Parameter(noise)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.noise\n",
    "\n",
    "\n",
    "def diffusion_pipe(noise_module: LatentNoise, num_inference_steps):\n",
    "        z = noise_module()\n",
    "        for i in range(start_step, num_inference_steps):\n",
    "            t = scheduler.timesteps[i]\n",
    "            z = scheduler.scale_model_input(z, t)\n",
    "            with torch.no_grad():\n",
    "                noise_pred = checkpointed_unet(z, t)[\"sample\"]\n",
    "            z = scheduler.step(noise_pred, t, z).prev_sample\n",
    "            z0 = scheduler.step(noise_pred, t, z).pred_original_sample\n",
    "        return z, z0\n",
    "\n",
    "def plot_images(images, titles=None, figsize=(50, 5), save_path=None):\n",
    "    n = len(images)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(n*5, 5))\n",
    "\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    #for i, img in enumerate(images):\n",
    "    #    img.save(f\"{save_path}/{i}.png\")\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "        #if titles is not None:\n",
    "        #    axes[i].set_title(titles[i])\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    #plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_to_pil(tensor):\n",
    "    image = tensor.cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    image = PIL.Image.fromarray(np.array(image[0].detach().numpy() * 255).astype(np.uint8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def tensor_to_pil_image(tensor):\n",
    "    image = tensor.cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    image = PIL.Image.fromarray(np.array(image[0].detach().numpy() * 255).astype(np.uint8))\n",
    "    return image\n",
    "\n",
    "# conditional sampling\n",
    "num_inference_steps = 100\n",
    "start_step = 20\n",
    "\n",
    "\n",
    "for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "    #plot_to_pil(batch['img'])\n",
    "    with torch.no_grad():\n",
    "        z = vqvae.encode(batch['img'].to(device))   # encode the image in the latent space\n",
    "    z = z.latents\n",
    "    #plot_to_pil(z)\n",
    "    \n",
    "    #cond = z.view(1,-1)\n",
    "    #cond = normalize(cond)\n",
    "    #cond = cond + 0.5 * math.sqrt(512) * classifier.fc1.weight[31].unsqueeze(0)\n",
    "    #cond = denormalize(cond)\n",
    "    #z = cond.view(1,3,64,64)\n",
    "    #dec_z = vqvae.decode(z)[0]\n",
    "    #plot_to_pil(dec_z)\n",
    "    \n",
    "    inverted_latents = invert(z, num_inference_steps)                  # do the ddim scheduler reversed to add noise to the latents\n",
    "    z = inverted_latents[-(start_step + 1)].unsqueeze(0)                  # use these latents to start the sampling. better performance when using not the last latent sample\n",
    "    #plot_to_pil(z)\n",
    "    noise_module = LatentNoise(z.clone()).to(device)                    # convert latent noise to a parameter module for optimization\n",
    "    noise_module.noise.requires_grad = True\n",
    "    intermediate_results = [batch['img'].to(device)]   # list to store the results of the steering\n",
    "    intermediate_preds = [round(classifier(batch['img'].to(device))[0][cls_id].item(), 5)]\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        noise_module.parameters(), lr=0.01, maximize=False # not minimize gradient ascent\n",
    "    )\n",
    "    learning_scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "    \n",
    "    x = torch.zeros_like(z)\n",
    "    current_loss = float('inf')\n",
    "    preds_binary = False\n",
    "    current_pred = 0.0\n",
    "    i = 0\n",
    "    # for the linear classifier it works perfect to break out of the loop if the prediction switches.\n",
    "    while (current_pred < 1.0) & (i < 20) :\n",
    "    #while (preds_binary == False) & (i < 20) :\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            x, x0 = diffusion_pipe(noise_module, num_inference_steps) # forward\n",
    "            #plot_to_pil(x)\n",
    "            decoded_x = vqvae.decode(x)[0]\n",
    "            #plot_to_pil(decoded_x)\n",
    "            current_pred = classifier(decoded_x)[0][cls_id]\n",
    "\n",
    "            if i % 1 == 0:\n",
    "                intermediate_results.append(decoded_x)\n",
    "                intermediate_preds.append(round(current_pred.item(), 5))\n",
    "\n",
    "            loss, preds_binary = classifier_loss(classifier, decoded_x, 1.0, i)\n",
    "            l1_dist = minDist_loss(decoded_x, batch['img'].to(device))\n",
    "            loss += l1_dist * 10\n",
    "            \n",
    "            if i % 2 == 0:\n",
    "                print(i, \"loss:\", loss.item(), \"lr:\", learning_scheduler.get_lr(), \"l1-dist\", l1_dist.item())\n",
    "                #print(i, \"loss:\", loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            learning_scheduler.step()\n",
    "\n",
    "            current_loss = loss.item()\n",
    "            i += 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image = vqvae.decode(x)[0]\n",
    "\n",
    "    print(f\"Diffusion Counterfactual generated with loss: {current_loss} | classifier_prediction: {current_pred} | l1_dist: {l1_dist} | in {i} optimization steps\")\n",
    "    \n",
    "    #lrp after manipulation\n",
    "    attr_znt_0 = [xai_zennit(classifier, image, RuleComposite=LayerMapComposite(layer_map_lrp_0), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    attr_znt_eps = [xai_zennit(classifier, image, RuleComposite=LayerMapComposite(layer_map_lrp_eps), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    attr_znt_zplus = [xai_zennit(classifier, image, RuleComposite=LayerMapComposite(layer_map_lrp_zplus), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    show_attributions(directory_names[step], attr_znt_0, title='Post Zennit LRP-0')\n",
    "    show_attributions(directory_names[step], attr_znt_eps, title='Post Zennit LRP-EPS')\n",
    "    show_attributions(directory_names[step], attr_znt_zplus, title='Post Zennit LRP-Z+')\n",
    "    image.requires_grad = False\n",
    "    \n",
    "    images = [tensor_to_pil_image(tensor) for tensor in intermediate_results]\n",
    "    gif_path = f\"{directory_names[step]}/GIF.gif\"\n",
    "    imageio.mimsave(gif_path, images, format='GIF', duration=2.0, loop=0)  # duration is in seconds\n",
    "\n",
    "    row_path = f\"{directory_names[step]}/sequence.png\"\n",
    "    plot_images(images, intermediate_preds, save_path=row_path)\n",
    "\n",
    "    # process image\n",
    "    image_processed = image.cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    image_pil = PIL.Image.fromarray(np.array(image_processed[0] * 255).astype(np.uint8))\n",
    "    ori_processed = batch['img'].cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    ori_image = PIL.Image.fromarray(np.array(ori_processed[0] * 255).astype(np.uint8))\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(image_pil)\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title('Diffusion Counterfactual Image')\n",
    "    axs[1].imshow(ori_image)\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title('Original Image')\n",
    "    #plt.show()\n",
    "    fig.savefig(f'{directory_names[step]}/ori_vs_DCE.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    image_pil.save(f\"{directory_names[step]}/diffCounter_IMG.png\")\n",
    "    print('finish')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffcounter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
