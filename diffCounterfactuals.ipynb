{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_ResNet50/folder_IMG_27300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dai/GPU-Student-2/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/dai/GPU-Student-2/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/diffusers/configuration_utils.py:244: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddim.DDIMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "The config attributes {'timestep_values': None, 'timesteps': 1000} were passed to DDIMScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n",
      "/home/dai/GPU-Student-2/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dai/GPU-Student-2/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.5639, device='cuda:0')\n",
      "tensor([False])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/home/dai/GPU-Student-2/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "100%|██████████| 99/99 [00:02<00:00, 45.22it/s]\n",
      "/home/dai/GPU-Student-2/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -2.6833388805389404\n",
      "0 loss: 4.0918049812316895 lr: [0.01] l1-dist 0.040846627205610275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dai/GPU-Student-2/Cederic/anaconda3/envs/diffcounter/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:855: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.974537968635559\n",
      "2 loss: 3.460402488708496 lr: [0.009931100837462445] l1-dist 0.048586469143629074\n",
      "Classifier prediction: -1.4567852020263672\n",
      "4 loss: 2.896562337875366 lr: [0.009774869058090914] l1-dist 0.04397771880030632\n",
      "Classifier prediction: -1.0457684993743896\n",
      "6 loss: 2.48736310005188 lr: [0.009543642776065642] l1-dist 0.044159453362226486\n",
      "Classifier prediction: -0.25201019644737244\n",
      "8 loss: 1.7349553108215332 lr: [0.009241066670644704] l1-dist 0.048294514417648315\n",
      "Classifier prediction: 0.20791363716125488\n",
      "10 loss: 1.3454234600067139 lr: [0.008871910576983217] l1-dist 0.05533371493220329\n",
      "Classifier prediction: 0.4364343583583832\n",
      "12 loss: 1.1917204856872559 lr: [0.008441994226264132] l1-dist 0.06281548738479614\n",
      "Classifier prediction: 0.8920412063598633\n",
      "14 loss: 0.7447999119758606 lr: [0.007958095421518055] l1-dist 0.06368411332368851\n",
      "Diffusion Counterfactual generated with loss: 0.7876085042953491 | classifier_prediction: 1.1598352193832397 | l1_dist: 0.0627773255109787 | in 16 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:32<00:00, 32.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import PIL.Image\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import imageio\n",
    "import math\n",
    "import os\n",
    "\n",
    "from diffusers import UNet2DModel, DDIMScheduler, VQModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from zennit.composites import LayerMapComposite\n",
    "from zennit.rules import Epsilon, ZPlus, Pass, Norm\n",
    "\n",
    "from data.dataset import ImageDataset, CelebHQAttrDataset\n",
    "from init_classifier import LinearClassifier, VQVAEClassifier, ResNet50Classifier\n",
    "from xai_lrp import xai_zennit, show_attributions\n",
    "\n",
    "\n",
    "class CheckpointedUNetWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(CheckpointedUNetWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def checkpointed_forward(self, module, *inputs):\n",
    "        def custom_forward(*inputs):\n",
    "            return module(*inputs)\n",
    "        return checkpoint(custom_forward, *inputs)\n",
    "\n",
    "    def forward(self, sample, timestep):\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps * torch.ones(sample.shape[0], dtype=timesteps.dtype, device=timesteps.device)\n",
    "\n",
    "        t_emb = self.model.time_proj(timesteps)\n",
    "        #t_emb = t_emb.to(dtype=self.dtype)\n",
    "        emb = self.model.time_embedding(t_emb)\n",
    "\n",
    "        # 2. pre-process\n",
    "        skip_sample = sample\n",
    "        sample = self.model.conv_in(sample)\n",
    "\n",
    "        # 3. down\n",
    "        down_block_res_samples = (sample,)\n",
    "        for downsample_block in self.model.down_blocks:\n",
    "            if hasattr(downsample_block, \"skip_conv\"):\n",
    "                sample, res_samples, skip_sample = self.checkpointed_forward(downsample_block, sample, emb, skip_sample)\n",
    "            else:\n",
    "                sample, res_samples = self.checkpointed_forward(downsample_block, sample, emb)\n",
    "\n",
    "            down_block_res_samples += res_samples\n",
    "        \n",
    "        # 4. mid\n",
    "        sample = self.checkpointed_forward(self.model.mid_block, sample, emb)\n",
    "\n",
    "        # 5. up\n",
    "        skip_sample = None\n",
    "        for upsample_block in self.model.up_blocks:\n",
    "            res_samples = down_block_res_samples[-len(upsample_block.resnets) :]\n",
    "            down_block_res_samples = down_block_res_samples[: -len(upsample_block.resnets)]\n",
    "\n",
    "            if hasattr(upsample_block, \"skip_conv\"):\n",
    "                sample, skip_sample = self.checkpointed_forward(upsample_block, sample, res_samples, emb, skip_sample)\n",
    "            else:\n",
    "                sample = self.checkpointed_forward(upsample_block, sample, res_samples, emb)\n",
    "\n",
    "        # 6. post-process\n",
    "        sample = self.model.conv_norm_out(sample)\n",
    "        sample = self.model.conv_act(sample)\n",
    "        sample = self.model.conv_out(sample)\n",
    "\n",
    "        if skip_sample is not None:\n",
    "            sample += skip_sample\n",
    "\n",
    "        return {\"sample\": sample}\n",
    "\n",
    "def classifier_loss(classifier, images, targets, idx):\n",
    "    preds = classifier(images)\n",
    "    if idx % 2 == 0:\n",
    "        print(f\"Classifier prediction: {preds[0][cls_id]}\")\n",
    "    targets = torch.tensor(targets).to(device)\n",
    "    #error = torch.nn.functional.binary_cross_entropy_with_logits(preds[0][31], targets)\n",
    "    error = torch.abs(preds[0][cls_id] - targets).mean()\n",
    "    preds_binary = torch.sigmoid(preds[0][cls_id]) > 0.5    \n",
    "\n",
    "    return error, preds_binary\n",
    "\n",
    "def minDist_loss(images, original_images):\n",
    "    error = torch.abs(images - original_images).mean()\n",
    "    return error\n",
    "\n",
    "\n",
    "# data loading with ground truth no smiling\n",
    "data = ImageDataset('/home/dai/GPU-Student-2/Cederic/DataSciPro/data/misclsData_gt1', image_size=256, exts=['jpg', 'JPG', 'png'], do_augment=False, sort_names=True)\n",
    "dataloader = DataLoader(data, batch_size=1, shuffle=False)\n",
    "\n",
    "# create output folders\n",
    "directory_names = []\n",
    "for i, _ in enumerate(dataloader):\n",
    "    img_index = dataloader.dataset.paths[i].name.split('_')[0]\n",
    "    directory_name = os.path.join(\"/home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_Smiling_ResNet50\", f'folder_IMG_{img_index}')\n",
    "    directory_names.append(directory_name)\n",
    "    os.makedirs(directory_name, exist_ok=True)\n",
    "    print(f'Created directory: {directory_name}')\n",
    "\n",
    "#\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cls_type = 'res50'\n",
    "cls_id =  CelebHQAttrDataset.cls_to_id['Smiling']\n",
    "\n",
    "# load all models\n",
    "unet = UNet2DModel.from_pretrained(\"CompVis/ldm-celebahq-256\", subfolder=\"unet\")\n",
    "vqvae = VQModel.from_pretrained(\"CompVis/ldm-celebahq-256\", subfolder=\"vqvae\")\n",
    "scheduler = DDIMScheduler.from_config(\"CompVis/ldm-celebahq-256\", subfolder=\"scheduler\")\n",
    "\n",
    "unet.to(device)\n",
    "vqvae.to(device)\n",
    "\n",
    "checkpointed_unet = CheckpointedUNetWrapper(unet)\n",
    "\n",
    "# load all models\n",
    "if cls_type == 'linear':    \n",
    "    classifier = LinearClassifier.load_from_checkpoint(\"/home/dai/GPU-Student-2/Cederic/DataSciPro/cls_checkpoints/ffhq256.b128linear2024-06-02 13:08:28.ckpt\",\n",
    "                                            input_dim = data[0]['img'].shape,\n",
    "                                            num_classes = len(CelebHQAttrDataset.id_to_cls))\n",
    "elif cls_type == 'vqvae':\n",
    "    classifier = VQVAEClassifier.load_from_checkpoint(\"/home/dai/GPU-Student-2/Cederic/DataSciPro/cls_checkpoints/ffhq256.b32vqvae2024-06-01 08:48:59.ckpt\",\n",
    "                                           num_classes = len(CelebHQAttrDataset.id_to_cls))\n",
    "    \n",
    "elif cls_type == 'res50':\n",
    "    classifier = ResNet50Classifier.load_from_checkpoint(\"/home/dai/GPU-Student-2/Cederic/DataSciPro/cls_checkpoints/ffhq256.b64res502024-06-02 17:06:41.ckpt\",\n",
    "                                            num_classes = len(CelebHQAttrDataset.id_to_cls))\n",
    "\n",
    "classifier.to(device)\n",
    "classifier.eval()\n",
    "# check functionality of classifier\n",
    "all_outputs = []\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['img'].to(classifier.device)\n",
    "        outputs = classifier(inputs)\n",
    "        print(outputs[0][cls_id])\n",
    "\n",
    "        preds_binary = torch.sigmoid(outputs[:, cls_id].cpu()) > 0.5\n",
    "        all_outputs.append(preds_binary) \n",
    "all_outputs = torch.cat(all_outputs, dim=0)\n",
    "print(all_outputs)\n",
    "\n",
    "\n",
    "###### explainable ai lrp\n",
    "# lrp rules\n",
    "layer_map_lrp_0 = [\n",
    "    (torch.nn.ReLU, Pass()),  # ignore activations\n",
    "    (torch.nn.Linear, Epsilon(epsilon=0)),  # this is the dense Linear, not any Linear\n",
    "    (torch.nn.Conv2d, ZPlus()),\n",
    "    (torch.nn.BatchNorm2d, Pass()),\n",
    "    (torch.nn.AdaptiveAvgPool2d, Norm()),\n",
    "]\n",
    "\n",
    "layer_map_lrp_zplus = [\n",
    "    (torch.nn.ReLU, Pass()),\n",
    "    (torch.nn.Linear, ZPlus()),  # this is the dense Linear, not any Linear\n",
    "    (torch.nn.Conv2d, ZPlus()),\n",
    "    (torch.nn.BatchNorm2d, Pass()),\n",
    "    (torch.nn.AdaptiveAvgPool2d, Norm()),\n",
    "]\n",
    "\n",
    "layer_map_lrp_eps = [\n",
    "    (torch.nn.ReLU, Pass()),\n",
    "    (torch.nn.Linear, Epsilon(epsilon=1)),  # this is the dense Linear, not any Linear\n",
    "    (torch.nn.Conv2d, ZPlus()),\n",
    "    (torch.nn.BatchNorm2d, Pass()),\n",
    "    (torch.nn.AdaptiveAvgPool2d, Norm()),\n",
    "]\n",
    "\n",
    "#before manipulation\n",
    "for i, batch in enumerate(dataloader):\n",
    "    inputs = batch['img'].to(classifier.device)\n",
    "    attr_znt_0 = [xai_zennit(classifier, inputs, RuleComposite=LayerMapComposite(layer_map_lrp_0), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    attr_znt_eps = [xai_zennit(classifier, inputs, RuleComposite=LayerMapComposite(layer_map_lrp_eps), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    attr_znt_zplus = [xai_zennit(classifier, inputs, RuleComposite=LayerMapComposite(layer_map_lrp_zplus), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    show_attributions(directory_names[i], attr_znt_0, title='Pre Zennit LRP-0')\n",
    "    show_attributions(directory_names[i], attr_znt_eps, title='Pre Zennit LRP-EPS')\n",
    "    show_attributions(directory_names[i], attr_znt_zplus, title='Pre Zennit LRP-Z+')\n",
    "        \n",
    "## Inversion\n",
    "def invert(\n",
    "    start_latents,\n",
    "    num_inference_steps,\n",
    "    device=device,\n",
    "):\n",
    "\n",
    "    # Latents are now the specified start latents\n",
    "    latents = start_latents.clone()\n",
    "\n",
    "    # We'll keep a list of the inverted latents as the process goes on\n",
    "    intermediate_latents = []\n",
    "\n",
    "    # Set num inference steps\n",
    "    scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "\n",
    "    # Reversed timesteps <<<<<<<<<<<<<<<<<<<<\n",
    "    timesteps = reversed(scheduler.timesteps)\n",
    "\n",
    "    for i in tqdm(range(1, num_inference_steps), total=num_inference_steps - 1):\n",
    "\n",
    "        # We'll skip the final iteration\n",
    "        if i >= num_inference_steps - 1:\n",
    "            continue\n",
    "\n",
    "        t = timesteps[i]\n",
    "\n",
    "        # Expand the latents if we are doing classifier free guidance\n",
    "        latent_model_input = latents\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "        # Predict the noise residual\n",
    "        noise_pred = checkpointed_unet(latent_model_input, t)[\"sample\"]\n",
    "\n",
    "        current_t = max(0, t.item() - (1000 // num_inference_steps))  # t\n",
    "        next_t = t  # min(999, t.item() + (1000//num_inference_steps)) # t+1\n",
    "        alpha_t = scheduler.alphas_cumprod[current_t]\n",
    "        alpha_t_next = scheduler.alphas_cumprod[next_t]\n",
    "\n",
    "        # Inverted update step (re-arranging the update step to get x(t) (new latents) as a function of x(t-1) (current latents)\n",
    "        latents = (latents - (1 - alpha_t).sqrt() * noise_pred) * (alpha_t_next.sqrt() / alpha_t.sqrt()) + (\n",
    "            1 - alpha_t_next\n",
    "        ).sqrt() * noise_pred\n",
    "\n",
    "        # Store\n",
    "        intermediate_latents.append(latents)\n",
    "\n",
    "    return torch.cat(intermediate_latents)\n",
    "\n",
    "\n",
    "class LatentNoise(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The LatentNoise Module makes it easier to update the noise tensor with torch optimizers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, noise: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.noise = torch.nn.Parameter(noise)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.noise\n",
    "\n",
    "\n",
    "def diffusion_pipe(noise_module: LatentNoise, num_inference_steps):\n",
    "        z = noise_module()\n",
    "        for i in range(start_step, num_inference_steps):\n",
    "            t = scheduler.timesteps[i]\n",
    "            z = scheduler.scale_model_input(z, t)\n",
    "            with torch.no_grad():\n",
    "                noise_pred = checkpointed_unet(z, t)[\"sample\"]\n",
    "            z = scheduler.step(noise_pred, t, z).prev_sample\n",
    "            z0 = scheduler.step(noise_pred, t, z).pred_original_sample\n",
    "        return z, z0\n",
    "\n",
    "def plot_images(images, titles=None, figsize=(50, 5), save_path=None):\n",
    "    n = len(images)\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "        if titles is not None:\n",
    "            axes[i].set_title(titles[i])\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    #plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_to_pil(tensor):\n",
    "    image = tensor.cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    image = PIL.Image.fromarray(np.array(image[0].detach().numpy() * 255).astype(np.uint8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def tensor_to_pil_image(tensor):\n",
    "    image = tensor.cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    image = PIL.Image.fromarray(np.array(image[0].detach().numpy() * 255).astype(np.uint8))\n",
    "    return image\n",
    "\n",
    "# conditional sampling\n",
    "num_inference_steps = 100\n",
    "start_step = 20\n",
    "\n",
    "\n",
    "for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "    #plot_to_pil(batch['img'])\n",
    "    with torch.no_grad():\n",
    "        z = vqvae.encode(batch['img'].to(device))   # encode the image in the latent space\n",
    "    z = z.latents\n",
    "    \n",
    "    #cond = z.view(1,-1)\n",
    "    #cond = normalize(cond)\n",
    "    #cond = cond + 0.5 * math.sqrt(512) * classifier.fc1.weight[31].unsqueeze(0)\n",
    "    #cond = denormalize(cond)\n",
    "    #z = cond.view(1,3,64,64)\n",
    "    #dec_z = vqvae.decode(z)[0]\n",
    "    #plot_to_pil(dec_z)\n",
    "    \n",
    "    inverted_latents = invert(z, num_inference_steps)                  # do the ddim scheduler reversed to add noise to the latents\n",
    "    z = inverted_latents[-(start_step + 1)].unsqueeze(0)                  # use these latents to start the sampling. better performance when using not the last latent sample\n",
    "    noise_module = LatentNoise(z.clone()).to(device)                    # convert latent noise to a parameter module for optimization\n",
    "    noise_module.noise.requires_grad = True\n",
    "    intermediate_results = [batch['img'].to(device)]   # list to store the results of the steering\n",
    "    intermediate_preds = [round(classifier(batch['img'].to(device))[0][cls_id].item(), 5)]\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        noise_module.parameters(), lr=0.01, maximize=False # not minimize gradient ascent\n",
    "    )\n",
    "    learning_scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "    \n",
    "    x = torch.zeros_like(z)\n",
    "    current_loss = float('inf')\n",
    "    preds_binary = False\n",
    "    current_pred = 0.0\n",
    "    i = 0\n",
    "    while (current_pred < 1.0) & (i < 20) :\n",
    "    #for i in tqdm(range(num_optimization_steps)):\n",
    "            optimizer.zero_grad()\n",
    "            x, x0 = diffusion_pipe(noise_module, num_inference_steps) # forward\n",
    "            decoded_x = vqvae.decode(x)[0]\n",
    "            current_pred = classifier(decoded_x)[0][cls_id]\n",
    "\n",
    "            if i % 1 == 0:\n",
    "                intermediate_results.append(decoded_x)\n",
    "                intermediate_preds.append(round(current_pred.item(), 5))\n",
    "\n",
    "            loss, preds_binary = classifier_loss(classifier, decoded_x, 1.0, i)\n",
    "            l1_dist = minDist_loss(decoded_x, batch['img'].to(device))\n",
    "            loss += l1_dist * 10\n",
    "            \n",
    "            if i % 2 == 0:\n",
    "                print(i, \"loss:\", loss.item(), \"lr:\", learning_scheduler.get_lr(), \"l1-dist\", l1_dist.item())\n",
    "                #print(i, \"loss:\", loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            learning_scheduler.step()\n",
    "\n",
    "            current_loss = loss.item()\n",
    "            i += 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image = vqvae.decode(x)[0]\n",
    "\n",
    "    print(f\"Diffusion Counterfactual generated with loss: {current_loss} | classifier_prediction: {current_pred} | l1_dist: {l1_dist} | in {i} optimization steps\")\n",
    "    \n",
    "    #lrp after manipulation\n",
    "    attr_znt_0 = [xai_zennit(classifier, image, RuleComposite=LayerMapComposite(layer_map_lrp_0), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    attr_znt_eps = [xai_zennit(classifier, image, RuleComposite=LayerMapComposite(layer_map_lrp_eps), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    attr_znt_zplus = [xai_zennit(classifier, image, RuleComposite=LayerMapComposite(layer_map_lrp_zplus), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    show_attributions(directory_names[step], attr_znt_0, title='Post Zennit LRP-0')\n",
    "    show_attributions(directory_names[step], attr_znt_eps, title='Post Zennit LRP-EPS')\n",
    "    show_attributions(directory_names[step], attr_znt_zplus, title='Post Zennit LRP-Z+')\n",
    "    image.requires_grad = False\n",
    "    \n",
    "    images = [tensor_to_pil_image(tensor) for tensor in intermediate_results]\n",
    "    gif_path = f\"{directory_names[step]}/GIF.gif\"\n",
    "    imageio.mimsave(gif_path, images, format='GIF', duration=2.0, loop=0)  # duration is in seconds\n",
    "\n",
    "    row_path = f\"{directory_names[step]}/sequence.png\"\n",
    "    plot_images(images, intermediate_preds, save_path=row_path)\n",
    "\n",
    "    # process image\n",
    "    image_processed = image.cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    image_pil = PIL.Image.fromarray(np.array(image_processed[0] * 255).astype(np.uint8))\n",
    "    ori_processed = batch['img'].cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    ori_image = PIL.Image.fromarray(np.array(ori_processed[0] * 255).astype(np.uint8))\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(image_pil)\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title('Diffusion Counterfactual Image')\n",
    "    axs[1].imshow(ori_image)\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title('Original Image')\n",
    "    #plt.show()\n",
    "    fig.savefig(f'{directory_names[step]}/ori_vs_DCE.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    image_pil.save(f\"{directory_names[step]}/diffCounter_IMG.png\")\n",
    "    print('finish')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffcounter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
