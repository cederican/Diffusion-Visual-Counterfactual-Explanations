{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_27152\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_27291\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_28459\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_28510\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_28561\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_28641\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_28749\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_28754\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_28875\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_29056\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_29075\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_29224\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_29242\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_29263\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_29299\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_29300\n",
      "Created directory: /home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear/folder_IMG_29311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'timestep_values': None, 'timesteps': 1000} were passed to DDIMScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-3.0893, device='cuda:0')\n",
      "tensor(-2.2242, device='cuda:0')\n",
      "tensor(-1.4349, device='cuda:0')\n",
      "tensor(-3.0878, device='cuda:0')\n",
      "tensor(-0.5016, device='cuda:0')\n",
      "tensor(-0.9032, device='cuda:0')\n",
      "tensor(-1.8061, device='cuda:0')\n",
      "tensor(-1.5006, device='cuda:0')\n",
      "tensor(-1.5091, device='cuda:0')\n",
      "tensor(-3.0440, device='cuda:0')\n",
      "tensor(-1.7235, device='cuda:0')\n",
      "tensor(-2.6975, device='cuda:0')\n",
      "tensor(-1.1172, device='cuda:0')\n",
      "tensor(-0.9103, device='cuda:0')\n",
      "tensor(-1.3892, device='cuda:0')\n",
      "tensor(-0.6905, device='cuda:0')\n",
      "tensor(-1.6558, device='cuda:0')\n",
      "tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 41.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -3.2522175312042236\n",
      "0 loss: 5.699406623840332 lr: [0.01] l1-dist 0.0447189137339592\n",
      "Classifier prediction: -2.5887632369995117\n",
      "2 loss: 5.0674028396606445 lr: [0.009931100837462445] l1-dist 0.047863952815532684\n",
      "Classifier prediction: -2.0948095321655273\n",
      "4 loss: 4.670788764953613 lr: [0.009774869058090914] l1-dist 0.057597946375608444\n",
      "Classifier prediction: -1.5820859670639038\n",
      "6 loss: 4.230411052703857 lr: [0.009543642776065642] l1-dist 0.06483250856399536\n",
      "Classifier prediction: -1.0531268119812012\n",
      "8 loss: 3.791684150695801 lr: [0.009241066670644704] l1-dist 0.07385572046041489\n",
      "Classifier prediction: -0.42020437121391296\n",
      "10 loss: 3.2786717414855957 lr: [0.008871910576983217] l1-dist 0.08584672212600708\n",
      "Classifier prediction: 0.3648047149181366\n",
      "12 loss: 2.6706604957580566 lr: [0.008441994226264132] l1-dist 0.10354652255773544\n",
      "Diffusion Counterfactual generated with loss: 2.6706604957580566 | classifier_prediction: 0.3648047149181366 | l1_dist: 0.10354652255773544 | in 13 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1/17 [00:28<07:41, 28.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 42.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -2.318789482116699\n",
      "0 loss: 4.804568290710449 lr: [0.01] l1-dist 0.04857790097594261\n",
      "Classifier prediction: -1.6162564754486084\n",
      "2 loss: 4.228621006011963 lr: [0.009931100837462445] l1-dist 0.0612364336848259\n",
      "Classifier prediction: -1.0625545978546143\n",
      "4 loss: 3.7597155570983887 lr: [0.009774869058090914] l1-dist 0.06971611082553864\n",
      "Classifier prediction: -0.4725781977176666\n",
      "6 loss: 3.3060693740844727 lr: [0.009543642776065642] l1-dist 0.08334910869598389\n",
      "Classifier prediction: 0.15730783343315125\n",
      "8 loss: 2.8465020656585693 lr: [0.009241066670644704] l1-dist 0.10038098692893982\n",
      "Diffusion Counterfactual generated with loss: 2.8465020656585693 | classifier_prediction: 0.15730783343315125 | l1_dist: 0.10038098692893982 | in 9 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 2/17 [00:49<05:59, 23.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 42.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.474960207939148\n",
      "0 loss: 4.118856906890869 lr: [0.01] l1-dist 0.06438963860273361\n",
      "Classifier prediction: -0.9381029009819031\n",
      "2 loss: 3.522012233734131 lr: [0.009931100837462445] l1-dist 0.058390937745571136\n",
      "Classifier prediction: -0.3931589722633362\n",
      "4 loss: 3.070895195007324 lr: [0.009774869058090914] l1-dist 0.06777362525463104\n",
      "Classifier prediction: 0.05027990788221359\n",
      "6 loss: 2.6999075412750244 lr: [0.009543642776065642] l1-dist 0.0750187337398529\n",
      "Diffusion Counterfactual generated with loss: 2.6999075412750244 | classifier_prediction: 0.05027990788221359 | l1_dist: 0.0750187337398529 | in 7 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 3/17 [01:06<04:48, 20.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 42.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -3.186439275741577\n",
      "0 loss: 5.549551486968994 lr: [0.01] l1-dist 0.03631121292710304\n",
      "Classifier prediction: -2.5537922382354736\n",
      "2 loss: 4.967963218688965 lr: [0.009931100837462445] l1-dist 0.04141714423894882\n",
      "Classifier prediction: -1.954103946685791\n",
      "4 loss: 4.5201263427734375 lr: [0.009774869058090914] l1-dist 0.056602220982313156\n",
      "Classifier prediction: -1.4594134092330933\n",
      "6 loss: 4.130114555358887 lr: [0.009543642776065642] l1-dist 0.06707008183002472\n",
      "Classifier prediction: -0.7316893935203552\n",
      "8 loss: 3.4909865856170654 lr: [0.009241066670644704] l1-dist 0.07592970877885818\n",
      "Classifier prediction: 0.043284378945827484\n",
      "10 loss: 2.802475690841675 lr: [0.008871910576983217] l1-dist 0.08457601070404053\n",
      "Diffusion Counterfactual generated with loss: 2.802475690841675 | classifier_prediction: 0.043284378945827484 | l1_dist: 0.08457601070404053 | in 11 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 4/17 [01:30<04:47, 22.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 42.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -0.4957011044025421\n",
      "0 loss: 2.8765904903411865 lr: [0.01] l1-dist 0.03808894008398056\n",
      "Classifier prediction: 0.16523423790931702\n",
      "2 loss: 2.517380952835083 lr: [0.009931100837462445] l1-dist 0.0682615116238594\n",
      "Diffusion Counterfactual generated with loss: 2.517380952835083 | classifier_prediction: 0.16523423790931702 | l1_dist: 0.0682615116238594 | in 3 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 5/17 [01:39<03:27, 17.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 42.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -0.8376410007476807\n",
      "0 loss: 3.197890281677246 lr: [0.01] l1-dist 0.03602492809295654\n",
      "Classifier prediction: 0.08710312098264694\n",
      "2 loss: 2.442864418029785 lr: [0.009931100837462445] l1-dist 0.05299675464630127\n",
      "Diffusion Counterfactual generated with loss: 2.442864418029785 | classifier_prediction: 0.08710312098264694 | l1_dist: 0.05299675464630127 | in 3 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 6/17 [01:48<02:39, 14.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:01<00:00, 50.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.9193310737609863\n",
      "0 loss: 4.337862491607666 lr: [0.01] l1-dist 0.04185314103960991\n",
      "Classifier prediction: -1.1707054376602173\n",
      "2 loss: 3.65706205368042 lr: [0.009931100837462445] l1-dist 0.04863566905260086\n",
      "Classifier prediction: -0.5746570229530334\n",
      "4 loss: 3.4270753860473633 lr: [0.009774869058090914] l1-dist 0.08524185419082642\n",
      "Classifier prediction: 0.06140168756246567\n",
      "6 loss: 2.6542112827301025 lr: [0.009543642776065642] l1-dist 0.0715613067150116\n",
      "Diffusion Counterfactual generated with loss: 2.6542112827301025 | classifier_prediction: 0.06140168756246567 | l1_dist: 0.0715613067150116 | in 7 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 7/17 [02:02<02:23, 14.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:01<00:00, 49.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.6750441789627075\n",
      "0 loss: 4.079682350158691 lr: [0.01] l1-dist 0.04046384245157242\n",
      "Classifier prediction: -1.4992420673370361\n",
      "2 loss: 4.778306007385254 lr: [0.009931100837462445] l1-dist 0.12790636718273163\n",
      "Diffusion Counterfactual generated with loss: 3.102604866027832 | classifier_prediction: 0.5691964626312256 | l1_dist: 0.1671801209449768 | in 4 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 8/17 [02:11<01:54, 12.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:01<00:00, 49.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.632059097290039\n",
      "0 loss: 4.041840076446533 lr: [0.01] l1-dist 0.0409780852496624\n",
      "Classifier prediction: -0.6066117882728577\n",
      "2 loss: 3.2815661430358887 lr: [0.009931100837462445] l1-dist 0.0674954280257225\n",
      "Classifier prediction: 0.11276825517416\n",
      "4 loss: 2.6370694637298584 lr: [0.009774869058090914] l1-dist 0.07498377561569214\n",
      "Diffusion Counterfactual generated with loss: 2.6370694637298584 | classifier_prediction: 0.11276825517416 | l1_dist: 0.07498377561569214 | in 5 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 9/17 [02:22<01:37, 12.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:01<00:00, 49.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -3.263070583343506\n",
      "0 loss: 5.676961898803711 lr: [0.01] l1-dist 0.04138915613293648\n",
      "Classifier prediction: -2.706984519958496\n",
      "2 loss: 5.155869483947754 lr: [0.009931100837462445] l1-dist 0.04488849639892578\n",
      "Classifier prediction: -2.271000385284424\n",
      "4 loss: 4.784661293029785 lr: [0.009774869058090914] l1-dist 0.05136607214808464\n",
      "Classifier prediction: -1.8414192199707031\n",
      "6 loss: 4.42793083190918 lr: [0.009543642776065642] l1-dist 0.05865117162466049\n",
      "Classifier prediction: -1.4127620458602905\n",
      "8 loss: 4.081504821777344 lr: [0.009241066670644704] l1-dist 0.06687428802251816\n",
      "Classifier prediction: -1.0359368324279785\n",
      "10 loss: 3.7867374420166016 lr: [0.008871910576983217] l1-dist 0.07508005946874619\n",
      "Classifier prediction: 3.3919155597686768\n",
      "12 loss: 2.9619665145874023 lr: [0.008441994226264132] l1-dist 0.15700508654117584\n",
      "Diffusion Counterfactual generated with loss: 2.9619665145874023 | classifier_prediction: 3.3919155597686768 | l1_dist: 0.15700508654117584 | in 13 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 10/17 [02:46<01:51, 15.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:01<00:00, 49.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.825016736984253\n",
      "0 loss: 4.185042858123779 lr: [0.01] l1-dist 0.036002591252326965\n",
      "Classifier prediction: -1.3805158138275146\n",
      "2 loss: 3.9582600593566895 lr: [0.009931100837462445] l1-dist 0.057774417102336884\n",
      "Classifier prediction: -1.0711637735366821\n",
      "4 loss: 3.7886581420898438 lr: [0.009774869058090914] l1-dist 0.07174943387508392\n",
      "Classifier prediction: -0.6118988394737244\n",
      "6 loss: 3.451056480407715 lr: [0.009543642776065642] l1-dist 0.08391577005386353\n",
      "Classifier prediction: -0.047563232481479645\n",
      "8 loss: 2.9331984519958496 lr: [0.009241066670644704] l1-dist 0.08856351673603058\n",
      "Diffusion Counterfactual generated with loss: 2.7423624992370605 | classifier_prediction: 0.17037460207939148 | l1_dist: 0.0912737250328064 | in 10 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 11/17 [03:06<01:42, 17.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 42.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -2.889946460723877\n",
      "0 loss: 5.2585601806640625 lr: [0.01] l1-dist 0.036861352622509\n",
      "Classifier prediction: -1.5258978605270386\n",
      "2 loss: 4.340461730957031 lr: [0.009931100837462445] l1-dist 0.08145636320114136\n",
      "Classifier prediction: -0.1502390205860138\n",
      "4 loss: 3.2153098583221436 lr: [0.009774869058090914] l1-dist 0.10650709271430969\n",
      "Diffusion Counterfactual generated with loss: 2.837104320526123 | classifier_prediction: 0.3048820197582245 | l1_dist: 0.11419864743947983 | in 6 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 12/17 [03:19<01:18, 15.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:01<00:00, 49.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.1418135166168213\n",
      "0 loss: 3.5043153762817383 lr: [0.01] l1-dist 0.03625018522143364\n",
      "Classifier prediction: -0.5309040546417236\n",
      "2 loss: 3.130749225616455 lr: [0.009931100837462445] l1-dist 0.05998452752828598\n",
      "Classifier prediction: -0.026737965643405914\n",
      "4 loss: 2.7229793071746826 lr: [0.009774869058090914] l1-dist 0.06962413340806961\n",
      "Diffusion Counterfactual generated with loss: 2.527459144592285 | classifier_prediction: 0.2074051797389984 | l1_dist: 0.07348643243312836 | in 6 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 13/17 [03:31<00:59, 14.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 49.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -0.643534779548645\n",
      "0 loss: 3.346733570098877 lr: [0.01] l1-dist 0.07031987607479095\n",
      "Diffusion Counterfactual generated with loss: 1.7437050342559814 | classifier_prediction: 1.39066481590271 | l1_dist: 0.1134369820356369 | in 2 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 14/17 [03:37<00:36, 12.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 49.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.447609782218933\n",
      "0 loss: 3.9278740882873535 lr: [0.01] l1-dist 0.048026423901319504\n",
      "Classifier prediction: -0.043774642050266266\n",
      "2 loss: 2.6878087520599365 lr: [0.009931100837462445] l1-dist 0.06440342217683792\n",
      "Diffusion Counterfactual generated with loss: 2.326993942260742 | classifier_prediction: 0.4459528625011444 | l1_dist: 0.07729469239711761 | in 4 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 15/17 [03:47<00:22, 11.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 49.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -0.7701396346092224\n",
      "0 loss: 3.167670726776123 lr: [0.01] l1-dist 0.03975310176610947\n",
      "Classifier prediction: -0.21826401352882385\n",
      "2 loss: 2.6434991359710693 lr: [0.009931100837462445] l1-dist 0.04252350330352783\n",
      "Classifier prediction: 0.1932341754436493\n",
      "4 loss: 2.4113826751708984 lr: [0.009774869058090914] l1-dist 0.060461681336164474\n",
      "Diffusion Counterfactual generated with loss: 2.4113826751708984 | classifier_prediction: 0.1932341754436493 | l1_dist: 0.060461681336164474 | in 5 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 16/17 [03:58<00:11, 11.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:02<00:00, 49.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier prediction: -1.8123350143432617\n",
      "0 loss: 4.136782646179199 lr: [0.01] l1-dist 0.032444775104522705\n",
      "Classifier prediction: -0.7623562216758728\n",
      "2 loss: 3.3204517364501953 lr: [0.009931100837462445] l1-dist 0.05580954998731613\n",
      "Classifier prediction: -0.04490021616220474\n",
      "4 loss: 2.9405124187469482 lr: [0.009774869058090914] l1-dist 0.08956122398376465\n",
      "Diffusion Counterfactual generated with loss: 2.7100725173950195 | classifier_prediction: 0.31074413657188416 | l1_dist: 0.10208168625831604 | in 6 optimization steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [04:10<00:00, 14.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import PIL.Image\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import os\n",
    "\n",
    "from diffusers import UNet2DModel, DDIMScheduler, VQModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from pytorch_msssim import ssim, ms_ssim\n",
    "\n",
    "from zennit.composites import LayerMapComposite\n",
    "from zennit.rules import Epsilon, ZPlus, Pass, Norm\n",
    "\n",
    "from data.dataset import ImageDataset, CelebHQAttrDataset\n",
    "from classifier.train_classifier import LinearClassifier, ResNet50Classifier\n",
    "from util.xai_lrp import xai_zennit, show_attributions\n",
    "\n",
    "\n",
    "class CheckpointedUNetWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(CheckpointedUNetWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def checkpointed_forward(self, module, *inputs):\n",
    "        def custom_forward(*inputs):\n",
    "            return module(*inputs)\n",
    "        return checkpoint(custom_forward, *inputs)\n",
    "\n",
    "    def forward(self, sample, timestep):\n",
    "\n",
    "        # 1. time\n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)\n",
    "        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        timesteps = timesteps * torch.ones(sample.shape[0], dtype=timesteps.dtype, device=timesteps.device)\n",
    "\n",
    "        t_emb = self.model.time_proj(timesteps)\n",
    "        #t_emb = t_emb.to(dtype=self.dtype)\n",
    "        emb = self.model.time_embedding(t_emb)\n",
    "\n",
    "        # 2. pre-process\n",
    "        skip_sample = sample\n",
    "        sample = self.model.conv_in(sample)\n",
    "\n",
    "        # 3. down\n",
    "        down_block_res_samples = (sample,)\n",
    "        for downsample_block in self.model.down_blocks:\n",
    "            if hasattr(downsample_block, \"skip_conv\"):\n",
    "                sample, res_samples, skip_sample = self.checkpointed_forward(downsample_block, sample, emb, skip_sample)\n",
    "            else:\n",
    "                sample, res_samples = self.checkpointed_forward(downsample_block, sample, emb)\n",
    "\n",
    "            down_block_res_samples += res_samples\n",
    "\n",
    "        # 4. mid\n",
    "        sample = self.checkpointed_forward(self.model.mid_block, sample, emb)\n",
    "\n",
    "        # 5. up\n",
    "        skip_sample = None\n",
    "        for upsample_block in self.model.up_blocks:\n",
    "            res_samples = down_block_res_samples[-len(upsample_block.resnets) :]\n",
    "            down_block_res_samples = down_block_res_samples[: -len(upsample_block.resnets)]\n",
    "\n",
    "            if hasattr(upsample_block, \"skip_conv\"):\n",
    "                sample, skip_sample = self.checkpointed_forward(upsample_block, sample, res_samples, emb, skip_sample)\n",
    "            else:\n",
    "                sample = self.checkpointed_forward(upsample_block, sample, res_samples, emb)\n",
    "\n",
    "        # 6. post-process\n",
    "        sample = self.model.conv_norm_out(sample)\n",
    "        sample = self.model.conv_act(sample)\n",
    "        sample = self.model.conv_out(sample)\n",
    "\n",
    "        if skip_sample is not None:\n",
    "            sample += skip_sample\n",
    "\n",
    "        return {\"sample\": sample}\n",
    "\n",
    "def classifier_loss(classifier, images, targets, idx):\n",
    "    preds = classifier(images)\n",
    "    if idx % 2 == 0:\n",
    "        print(f\"Classifier prediction: {preds[0][cls_id]}\")\n",
    "    targets = torch.tensor(targets).to(device)\n",
    "    #error = torch.nn.functional.binary_cross_entropy_with_logits(preds[0][31], targets)\n",
    "    error = torch.abs(preds[0][cls_id] - targets).mean()\n",
    "    preds_binary = torch.sigmoid(preds[0][cls_id]) > 0.5\n",
    "\n",
    "    return error, preds_binary\n",
    "\n",
    "def minDist_loss(counterfactual_images, original_images):\n",
    "    # l1 distance\n",
    "    error = torch.abs(counterfactual_images - original_images).mean()\n",
    "\n",
    "    # ssim distance\n",
    "    # normalize images\n",
    "    #original_images = (original_images + 1) / 2\n",
    "    #counterfactual_images = (counterfactual_images + 1) / 2\n",
    "    #error = 1 - ssim(original_images, counterfactual_images, data_range=1.0, size_average=True)\n",
    "    return error\n",
    "\n",
    "\n",
    "# data loading with ground truth no smiling\n",
    "data = ImageDataset('/home/dai/GPU-Student-2/Cederic/DataSciPro/data/misclsData_gt0', image_size=256, exts=['jpg', 'JPG', 'png'], do_augment=False, sort_names=True)\n",
    "dataloader = DataLoader(data, batch_size=1, shuffle=False)\n",
    "\n",
    "# create output folders\n",
    "directory_names = []\n",
    "for i, _ in enumerate(dataloader):\n",
    "    img_index = dataloader.dataset.paths[i].name.split('_')[0]\n",
    "    directory_name = os.path.join(\"/home/dai/GPU-Student-2/Cederic/DataSciPro/data_output_bad_linear\", f'folder_IMG_{img_index}')\n",
    "    directory_names.append(directory_name)\n",
    "    os.makedirs(directory_name, exist_ok=True)\n",
    "    print(f'Created directory: {directory_name}')\n",
    "\n",
    "#\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cls_type = 'linear'\n",
    "cls_id =  CelebHQAttrDataset.cls_to_id['Smiling']\n",
    "\n",
    "# load all models\n",
    "unet = UNet2DModel.from_pretrained(\"CompVis/ldm-celebahq-256\", subfolder=\"unet\")\n",
    "vqvae = VQModel.from_pretrained(\"CompVis/ldm-celebahq-256\", subfolder=\"vqvae\")\n",
    "scheduler = DDIMScheduler.from_config(\"CompVis/ldm-celebahq-256\", subfolder=\"scheduler\")\n",
    "\n",
    "unet.to(device)\n",
    "vqvae.to(device)\n",
    "\n",
    "checkpointed_unet = CheckpointedUNetWrapper(unet)\n",
    "\n",
    "# load all models\n",
    "if cls_type == 'linear':    \n",
    "    classifier = LinearClassifier.load_from_checkpoint(\"/home/dai/GPU-Student-2/Cederic/DataSciPro/cls_checkpoints/ffhq256.b128linear2024-06-02 13:08:28.ckpt\",\n",
    "                                            input_dim = data[0]['img'].shape,\n",
    "                                            num_classes = len(CelebHQAttrDataset.id_to_cls))\n",
    "elif cls_type == 'res50':\n",
    "    classifier = ResNet50Classifier.load_from_checkpoint(\"/home/dai/GPU-Student-2/Cederic/DataSciPro/cls_checkpoints/ffhq256.b64res502024-06-02 17:06:41.ckpt\",\n",
    "                                            num_classes = len(CelebHQAttrDataset.id_to_cls))\n",
    "\n",
    "\n",
    "classifier.to(device)\n",
    "classifier.eval()\n",
    "# check functionality of classifier\n",
    "all_outputs = []\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['img'].to(classifier.device)\n",
    "        outputs = classifier(inputs)\n",
    "        print(outputs[0][cls_id])\n",
    "\n",
    "        preds_binary = torch.sigmoid(outputs[:, cls_id].cpu()) > 0.5\n",
    "        all_outputs.append(preds_binary) \n",
    "all_outputs = torch.cat(all_outputs, dim=0)\n",
    "print(all_outputs)\n",
    "\n",
    "\n",
    "###### explainable ai lrp\n",
    "# lrp rules\n",
    "layer_map_lrp_0 = [\n",
    "    (torch.nn.ReLU, Pass()),  # ignore activations\n",
    "    (torch.nn.Linear, Epsilon(epsilon=0)),  # this is the dense Linear, not any Linear\n",
    "    (torch.nn.Conv2d, ZPlus()),\n",
    "    (torch.nn.BatchNorm2d, Pass()),\n",
    "    (torch.nn.AdaptiveAvgPool2d, Norm()),\n",
    "]\n",
    "\n",
    "layer_map_lrp_zplus = [\n",
    "    (torch.nn.ReLU, Pass()),\n",
    "    (torch.nn.Linear, ZPlus()),  # this is the dense Linear, not any Linear\n",
    "    (torch.nn.Conv2d, ZPlus()),\n",
    "    (torch.nn.BatchNorm2d, Pass()),\n",
    "    (torch.nn.AdaptiveAvgPool2d, Norm()),\n",
    "]\n",
    "\n",
    "layer_map_lrp_eps = [\n",
    "    (torch.nn.ReLU, Pass()),\n",
    "    (torch.nn.Linear, Epsilon(epsilon=1)),  # this is the dense Linear, not any Linear\n",
    "    (torch.nn.Conv2d, ZPlus()),\n",
    "    (torch.nn.BatchNorm2d, Pass()),\n",
    "    (torch.nn.AdaptiveAvgPool2d, Norm()),\n",
    "]\n",
    "\n",
    "#before manipulation\n",
    "for i, batch in enumerate(dataloader):\n",
    "    inputs = batch['img'].to(classifier.device)\n",
    "    attr_znt_0 = [xai_zennit(classifier, inputs, RuleComposite=LayerMapComposite(layer_map_lrp_0), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    attr_znt_eps = [xai_zennit(classifier, inputs, RuleComposite=LayerMapComposite(layer_map_lrp_eps), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    attr_znt_zplus = [xai_zennit(classifier, inputs, RuleComposite=LayerMapComposite(layer_map_lrp_zplus), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    show_attributions(directory_names[i], attr_znt_0, title='Pre LRP-0')\n",
    "    #show_attributions(directory_names[i], attr_znt_eps, title='Pre LRP-EPS')\n",
    "    #show_attributions(directory_names[i], attr_znt_zplus, title='Pre LRP-Z+')\n",
    "        \n",
    "## Inversion\n",
    "def invert(\n",
    "    start_latents,\n",
    "    num_inference_steps,\n",
    "    device=device,\n",
    "):\n",
    "\n",
    "    # Latents are now the specified start latents\n",
    "    latents = start_latents.clone()\n",
    "\n",
    "    # We'll keep a list of the inverted latents as the process goes on\n",
    "    intermediate_latents = []\n",
    "\n",
    "    # Set num inference steps\n",
    "    scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "\n",
    "    # Reversed timesteps <<<<<<<<<<<<<<<<<<<<\n",
    "    timesteps = reversed(scheduler.timesteps)\n",
    "\n",
    "    for i in tqdm(range(1, num_inference_steps), total=num_inference_steps - 1):\n",
    "\n",
    "        # We'll skip the final iteration\n",
    "        if i >= num_inference_steps - 1:\n",
    "            continue\n",
    "\n",
    "        t = timesteps[i]\n",
    "\n",
    "        # Expand the latents if we are doing classifier free guidance\n",
    "        latent_model_input = latents\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "        # Predict the noise residual\n",
    "        noise_pred = checkpointed_unet(latent_model_input, t)[\"sample\"]\n",
    "\n",
    "        current_t = max(0, t.item() - (1000 // num_inference_steps))  # t\n",
    "        next_t = t  # min(999, t.item() + (1000//num_inference_steps)) # t+1\n",
    "        alpha_t = scheduler.alphas_cumprod[current_t]\n",
    "        alpha_t_next = scheduler.alphas_cumprod[next_t]\n",
    "\n",
    "        # Inverted update step (re-arranging the update step to get x(t) (new latents) as a function of x(t-1) (current latents)\n",
    "        latents = (latents - (1 - alpha_t).sqrt() * noise_pred) * (alpha_t_next.sqrt() / alpha_t.sqrt()) + (\n",
    "            1 - alpha_t_next\n",
    "        ).sqrt() * noise_pred\n",
    "\n",
    "        # Store\n",
    "        intermediate_latents.append(latents)\n",
    "\n",
    "    return torch.cat(intermediate_latents)\n",
    "\n",
    "\n",
    "class LatentNoise(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The LatentNoise Module makes it easier to update the noise tensor with torch optimizers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, noise: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.noise = torch.nn.Parameter(noise)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.noise\n",
    "\n",
    "\n",
    "def diffusion_pipe(noise_module: LatentNoise, num_inference_steps):\n",
    "        z = noise_module()\n",
    "        for i in range(start_step, num_inference_steps):\n",
    "            t = scheduler.timesteps[i]\n",
    "            z = scheduler.scale_model_input(z, t)\n",
    "            with torch.no_grad():\n",
    "                noise_pred = checkpointed_unet(z, t)[\"sample\"]\n",
    "            z = scheduler.step(noise_pred, t, z).prev_sample\n",
    "            z0 = scheduler.step(noise_pred, t, z).pred_original_sample\n",
    "        return z, z0\n",
    "\n",
    "def plot_images(images, titles=None, figsize=(50, 5), save_path=None):\n",
    "    n = len(images)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(n*5, 5))\n",
    "\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    #just for image sving\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        img.save(f\"{save_path}/{i}.png\")\n",
    "\n",
    "    #for i, img in enumerate(images):\n",
    "    #    axes[i].imshow(img)\n",
    "    #    axes[i].axis('off')\n",
    "    #    #if titles is not None:\n",
    "    #    #    axes[i].set_title(titles[i])\n",
    "#\n",
    "    #if save_path:\n",
    "    #    plt.savefig(save_path)\n",
    "    #plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_to_pil(tensor):\n",
    "    image = tensor.cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    image = PIL.Image.fromarray(np.array(image[0].detach().numpy() * 255).astype(np.uint8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def tensor_to_pil_image(tensor):\n",
    "    image = tensor.cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    image = PIL.Image.fromarray(np.array(image[0].detach().numpy() * 255).astype(np.uint8))\n",
    "    return image\n",
    "\n",
    "# conditional sampling\n",
    "num_inference_steps = 100\n",
    "start_step = 20\n",
    "\n",
    "\n",
    "for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "    #plot_to_pil(batch['img'])\n",
    "    with torch.no_grad():\n",
    "        z = vqvae.encode(batch['img'].to(device))   # encode the image in the latent space\n",
    "    z = z.latents\n",
    "    #plot_to_pil(z)\n",
    "    \n",
    "    #cond = z.view(1,-1)\n",
    "    #cond = normalize(cond)\n",
    "    #cond = cond + 0.5 * math.sqrt(512) * classifier.fc1.weight[31].unsqueeze(0)\n",
    "    #cond = denormalize(cond)\n",
    "    #z = cond.view(1,3,64,64)\n",
    "    #dec_z = vqvae.decode(z)[0]\n",
    "    #plot_to_pil(dec_z)\n",
    "    \n",
    "    inverted_latents = invert(z, num_inference_steps)                  # do the ddim scheduler reversed to add noise to the latents\n",
    "    z = inverted_latents[-(start_step + 1)].unsqueeze(0)                  # use these latents to start the sampling. better performance when using not the last latent sample\n",
    "    #plot_to_pil(z)\n",
    "    noise_module = LatentNoise(z.clone()).to(device)                    # convert latent noise to a parameter module for optimization\n",
    "    noise_module.noise.requires_grad = True\n",
    "    intermediate_results = [batch['img'].to(device)]   # list to store the results of the steering\n",
    "    intermediate_preds = [round(classifier(batch['img'].to(device))[0][cls_id].item(), 5)]\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        noise_module.parameters(), lr=0.01, maximize=False # not minimize gradient ascent\n",
    "    )\n",
    "    learning_scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)\n",
    "    \n",
    "    x = torch.zeros_like(z)\n",
    "    current_loss = float('inf')\n",
    "    preds_binary = False\n",
    "    current_pred = 0.0\n",
    "    i = 0\n",
    "    # for the linear classifier it works perfect to break out of the loop if the prediction switches.\n",
    "    #while (current_pred < 1.0) & (i < 20) :\n",
    "    while (preds_binary == False) & (i < 20) :\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            x, x0 = diffusion_pipe(noise_module, num_inference_steps) # forward\n",
    "            #plot_to_pil(x)\n",
    "            decoded_x = vqvae.decode(x)[0]\n",
    "            #plot_to_pil(decoded_x)\n",
    "            current_pred = classifier(decoded_x)[0][cls_id]\n",
    "\n",
    "            if i % 1 == 0:\n",
    "                intermediate_results.append(decoded_x)\n",
    "                intermediate_preds.append(round(current_pred.item(), 5))\n",
    "\n",
    "            loss, preds_binary = classifier_loss(classifier, decoded_x, 2.0, i)\n",
    "            l1_dist = minDist_loss(decoded_x, batch['img'].to(device))\n",
    "            #implementing the ssim and msssim distance\n",
    "            #ssim_dist = minDist_loss(decoded_x, batch['img'].to(device))\n",
    "\n",
    "            loss += l1_dist * 10\n",
    "            #loss += ssim_dist * 20\n",
    "            \n",
    "            if i % 2 == 0:\n",
    "                print(i, \"loss:\", loss.item(), \"lr:\", learning_scheduler.get_lr(), \"l1-dist\", l1_dist.item())\n",
    "                #print(i, \"loss:\", loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            learning_scheduler.step()\n",
    "\n",
    "            current_loss = loss.item()\n",
    "            i += 1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image = vqvae.decode(x)[0]\n",
    "\n",
    "    print(f\"Diffusion Counterfactual generated with loss: {current_loss} | classifier_prediction: {current_pred} | l1_dist: {l1_dist} | in {i} optimization steps\")\n",
    "    \n",
    "    #lrp after manipulation\n",
    "    attr_znt_0 = [xai_zennit(classifier, image, RuleComposite=LayerMapComposite(layer_map_lrp_0), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    attr_znt_eps = [xai_zennit(classifier, image, RuleComposite=LayerMapComposite(layer_map_lrp_eps), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    attr_znt_zplus = [xai_zennit(classifier, image, RuleComposite=LayerMapComposite(layer_map_lrp_zplus), device=device, target=torch.tensor(cls_id).to(device))[0]]\n",
    "    show_attributions(directory_names[step], attr_znt_0, title='Post LRP-0')\n",
    "    #show_attributions(directory_names[step], attr_znt_eps, title='Post LRP-EPS')\n",
    "    #show_attributions(directory_names[step], attr_znt_zplus, title='Post LRP-Z+')\n",
    "    image.requires_grad = False\n",
    "    \n",
    "    images = [tensor_to_pil_image(tensor) for tensor in intermediate_results]\n",
    "    gif_path = f\"{directory_names[step]}/GIF.gif\"\n",
    "    imageio.mimsave(gif_path, images, format='GIF', duration=10.0, loop=0)  # duration is in seconds\n",
    "\n",
    "    row_path = f\"{directory_names[step]}/sequence_small\"\n",
    "    plot_images(images, intermediate_preds, save_path=row_path)\n",
    "\n",
    "    # process image\n",
    "    image_processed = image.cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    image_pil = PIL.Image.fromarray(np.array(image_processed[0] * 255).astype(np.uint8))\n",
    "    ori_processed = batch['img'].cpu().permute(0, 2, 3, 1).clip(-1,1) * 0.5 + 0.5\n",
    "    ori_image = PIL.Image.fromarray(np.array(ori_processed[0] * 255).astype(np.uint8))\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(image_pil)\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title('Diffusion Counterfactual Image')\n",
    "    axs[1].imshow(ori_image)\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title('Original Image')\n",
    "    #plt.show()\n",
    "    fig.savefig(f'{directory_names[step]}/ori_vs_DCE.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "    image_pil.save(f\"{directory_names[step]}/diffCounter_IMG.png\")\n",
    "    print('finish')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffcounter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
